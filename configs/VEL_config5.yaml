encoder_configs:
  unspliced: {type: 'SparseTabularEncoder', num_embeddings: 36602  , max_tokens: 512}
  spliced: {type: 'SparseTabularEncoder', num_embeddings: 36602 , max_tokens: 512}
  velocity: {type: 'SparseTabularEncoder', num_embeddings: 36602 , max_tokens: 512}
modality_config:
  unspliced: {type: 'sequence', pad_len: 512, data_col_name: "indices", other_col: "data", pad_token: 0}
  spliced: {type: 'sequence', pad_len: 512,  data_col_name: "indices", other_col: "data", pad_token: 0}
  velocity: {type: 'sequence', pad_len: 512, data_col_name: "indices", other_col: "data", pad_token: 0}
num_fusion_tokens: 48
ds_frac: 0.2
predrop: false
batch_size: 4
seed: 42
lr: 0.0001
layers: 10 #12
wandb_name: "MFDOOM_Velocity"
lr_scheduler_type: "cosine" #"constant_with_warmup"
epochs: 16
#start_epoch: 22
bimodal_contrastive: True
non_fusion_fcl: True
fcl: True
fcl_root: [0,1,2]
fusion_combos: [3,2]
loss_masking: True
#inverse_doom: true
zorro: false
#output_dir: "training_output_22_02_13_12_2023"
#restart: "training_output_22_02_13_12_2023/21"
#restart_wandb: "0l846v73"
clip: 2.0
dataset: "/shared/fcaa53cd-ba57-4bfe-af9c-eaa958f95c1a_combined_all_veloc_sparse_processed"

