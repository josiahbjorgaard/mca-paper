{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67c52537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb954c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=load_from_disk('/shared/how2_all.dataset').with_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ca2f343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[392]['vid'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "887b0c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train string encoders\n",
    "en_data = [\" \".join(x) for x in dataset['en']]\n",
    "pt_data = [\" \".join(x) for x in dataset['pt']]\n",
    "sm_data = [\" \".join(x) for x in dataset['sm'] if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "22fbf871",
   "metadata": {},
   "outputs": [],
   "source": [
    "#en_training_corpus = (\n",
    "#    en_data[i : i + 1000]\n",
    "#    for i in range(0, len(en_data), 1000)\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "616c8efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "#old_tokenizer.encode(\" \".join(dataset[0]['sm']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0a94c4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer = BpeTrainer(special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "                    vocab_size=8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c25154ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(sm_data,trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "62f75b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save('sm_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d84db3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apped': 7302,\n",
       " 'calves': 7902,\n",
       " 'robe': 4426,\n",
       " 'crystal': 4703,\n",
       " 'fant': 5226,\n",
       " 'fortune': 4575,\n",
       " 'loveall': 5919,\n",
       " 'birdcage': 6483,\n",
       " 'beats': 2749,\n",
       " 'spread': 4128,\n",
       " 'bear': 1835,\n",
       " 'testing': 4828,\n",
       " 'suffering': 7187,\n",
       " 'outdoorsman': 7229,\n",
       " 'therap': 758,\n",
       " 'mouse': 4207,\n",
       " 'nicely': 7917,\n",
       " 'carpenter': 4389,\n",
       " 'feren': 3278,\n",
       " 'exer': 245,\n",
       " 'cir': 1033,\n",
       " 'things': 2047,\n",
       " 'mechan': 1241,\n",
       " 'colepsy': 7348,\n",
       " 'shelter': 7977,\n",
       " 'angelo': 8171,\n",
       " 'shine': 4412,\n",
       " 'pod': 2066,\n",
       " 'leading': 4344,\n",
       " 'dynam': 5716,\n",
       " 'apples': 6407,\n",
       " 'orchid': 7346,\n",
       " 'equine': 3059,\n",
       " 'instructor': 258,\n",
       " 'noseblunt': 7573,\n",
       " 'cu': 462,\n",
       " 'eliminate': 4377,\n",
       " 'word': 1586,\n",
       " 'ics': 604,\n",
       " 'comers': 7845,\n",
       " 'rate': 4108,\n",
       " 'nurse': 4030,\n",
       " 'sound': 1458,\n",
       " 'swee': 6844,\n",
       " 'intervie': 3008,\n",
       " 'garlic': 2293,\n",
       " 'planets': 7790,\n",
       " 'stands': 6477,\n",
       " 'scheduling': 3628,\n",
       " 'bloo': 5217,\n",
       " 'wondering': 7969,\n",
       " 'antiques': 3730,\n",
       " 'ank': 1342,\n",
       " 'cy': 537,\n",
       " 'peanut': 4727,\n",
       " 'cages': 5944,\n",
       " 'editor': 3481,\n",
       " 'listening': 3779,\n",
       " 'promin': 7864,\n",
       " 'interpretation': 4750,\n",
       " 'bolts': 3905,\n",
       " 'ght': 209,\n",
       " 'verde': 3007,\n",
       " 'campfire': 6180,\n",
       " 'professionally': 6774,\n",
       " 'language': 3492,\n",
       " 'rename': 7668,\n",
       " 'lea': 1042,\n",
       " 'faux': 4991,\n",
       " 'remedi': 6099,\n",
       " 'seeds': 5066,\n",
       " 'toning': 4569,\n",
       " 'modest': 5256,\n",
       " 'lineback': 6227,\n",
       " 'able': 402,\n",
       " 'forward': 1269,\n",
       " 'grace': 7040,\n",
       " 'steam': 6999,\n",
       " 'quali': 3142,\n",
       " 'quarterback': 7378,\n",
       " 'props': 4161,\n",
       " 'sweet': 4762,\n",
       " 'ko': 3861,\n",
       " 'durable': 6775,\n",
       " 'coordinator': 6928,\n",
       " 'onist': 4208,\n",
       " 'mexican': 3177,\n",
       " 'cosmetologist': 5094,\n",
       " 'insurance': 4518,\n",
       " 'rescu': 4613,\n",
       " 'peren': 5075,\n",
       " 'mechanic': 1398,\n",
       " 'meri': 5848,\n",
       " 'hopping': 4485,\n",
       " 'accents': 6293,\n",
       " 'tshirt': 7817,\n",
       " 'ned': 5413,\n",
       " 'spiced': 5117,\n",
       " '00': 2174,\n",
       " 'crested': 6020,\n",
       " 'window': 2696,\n",
       " 'elementary': 7641,\n",
       " 'runway': 3649,\n",
       " 'jewelry': 944,\n",
       " 'ped': 1163,\n",
       " 'yards': 8143,\n",
       " 'tortion': 6361,\n",
       " 'works': 1551,\n",
       " 'eling': 7150,\n",
       " 'cabbage': 6764,\n",
       " 'challenging': 5222,\n",
       " 'instrument': 1180,\n",
       " 'pulation': 4514,\n",
       " 'dentist': 2533,\n",
       " 'stock': 3419,\n",
       " 'productivity': 8012,\n",
       " 'saiidi': 5427,\n",
       " 'pizz': 6141,\n",
       " 'throwing': 1724,\n",
       " 'asa': 3370,\n",
       " 'windows': 1574,\n",
       " 'money': 1921,\n",
       " 'indust': 3472,\n",
       " 'gas': 5295,\n",
       " 'fiber': 5695,\n",
       " 'daily': 3352,\n",
       " 'achieve': 3244,\n",
       " 'surgeon': 5699,\n",
       " 'extend': 3660,\n",
       " 'make': 193,\n",
       " 'hoke': 3508,\n",
       " 'liar': 5165,\n",
       " 'racks': 5589,\n",
       " 'survival': 2867,\n",
       " 'cold': 2007,\n",
       " 'timing': 6527,\n",
       " 'ct': 214,\n",
       " 'communic': 792,\n",
       " 'celery': 4786,\n",
       " 'logical': 5420,\n",
       " 'addictions': 6157,\n",
       " 'anced': 947,\n",
       " 'wh': 131,\n",
       " 'dart': 5583,\n",
       " 'skier': 6645,\n",
       " 'sions': 2556,\n",
       " 'themed': 4705,\n",
       " 'pans': 6658,\n",
       " 'cess': 489,\n",
       " 'mathematics': 6653,\n",
       " 'bile': 2514,\n",
       " 'horses': 5592,\n",
       " 'shutter': 7874,\n",
       " 'screw': 3929,\n",
       " 'accessory': 5703,\n",
       " 'extern': 5044,\n",
       " 'fancy': 6487,\n",
       " 'master': 738,\n",
       " 'climb': 1202,\n",
       " 'cartridge': 5488,\n",
       " 'ance': 190,\n",
       " 'gammon': 7546,\n",
       " 'notes': 1753,\n",
       " 'stairs': 7678,\n",
       " 'studio': 1464,\n",
       " 'playful': 7780,\n",
       " 'ology': 3425,\n",
       " 'plus': 4145,\n",
       " 'shadow': 2659,\n",
       " 'lored': 3631,\n",
       " 'felting': 5861,\n",
       " 'windshield': 5960,\n",
       " 'pho': 318,\n",
       " 'supp': 872,\n",
       " 'hardest': 4878,\n",
       " 'crunch': 4901,\n",
       " 'japan': 2864,\n",
       " 'customers': 5486,\n",
       " 'doll': 4043,\n",
       " 'toss': 5040,\n",
       " 'dor': 4392,\n",
       " 'printmaking': 3154,\n",
       " 'chiropractor': 6890,\n",
       " 'insure': 7291,\n",
       " 'songs': 1789,\n",
       " 'tion': 109,\n",
       " 'mele': 4496,\n",
       " 'ings': 1945,\n",
       " 'ultra': 5615,\n",
       " 'leap': 8081,\n",
       " 'unt': 4839,\n",
       " 'fettuccine': 5097,\n",
       " 'ryo': 6282,\n",
       " 'vil': 3793,\n",
       " 'running': 1480,\n",
       " 'caul': 8008,\n",
       " 'space': 1332,\n",
       " 'aquarium': 2869,\n",
       " 'bio': 2052,\n",
       " 'lathe': 5210,\n",
       " 'an': 62,\n",
       " 'putty': 8022,\n",
       " 'weapon': 3336,\n",
       " 'reinstall': 6218,\n",
       " 'stren': 784,\n",
       " 'ac': 236,\n",
       " 'dressing': 2968,\n",
       " 'scientist': 5118,\n",
       " 'reference': 6940,\n",
       " 'pockets': 7951,\n",
       " 'system': 1396,\n",
       " 'appro': 1691,\n",
       " 'pine': 5586,\n",
       " 'are': 132,\n",
       " 'positions': 1819,\n",
       " 'ki': 460,\n",
       " 'cats': 2356,\n",
       " 'panels': 4169,\n",
       " 'examine': 5600,\n",
       " 'game': 521,\n",
       " 'spokes': 5902,\n",
       " 'penal': 7509,\n",
       " 'advantages': 5114,\n",
       " 'throat': 5818,\n",
       " 'tedori': 6297,\n",
       " 'mouthpieces': 6847,\n",
       " 'ox': 3937,\n",
       " 'front': 901,\n",
       " 'calorie': 7530,\n",
       " 'tian': 3603,\n",
       " 'importing': 7587,\n",
       " 'smelling': 6146,\n",
       " 'pots': 2492,\n",
       " 'hypnosis': 3148,\n",
       " 'triceps': 3541,\n",
       " 'woofer': 5024,\n",
       " 'egyptian': 2025,\n",
       " 'asing': 1864,\n",
       " 'ensures': 5747,\n",
       " 'rates': 6862,\n",
       " 'cail': 4770,\n",
       " 'editing': 2576,\n",
       " 'rain': 2929,\n",
       " 'salt': 4999,\n",
       " 'psycho': 2918,\n",
       " 'abstract': 5518,\n",
       " 'advancing': 4900,\n",
       " 'grips': 6036,\n",
       " 'cookie': 8031,\n",
       " 'dition': 674,\n",
       " 'meren': 4756,\n",
       " 'restaur': 5120,\n",
       " 'jewel': 902,\n",
       " 'length': 2756,\n",
       " 'sna': 1367,\n",
       " 'mosk': 6698,\n",
       " 'block': 1766,\n",
       " 'beau': 2145,\n",
       " 'pivo': 5481,\n",
       " 'spear': 7354,\n",
       " 'writer': 1497,\n",
       " 'removing': 1504,\n",
       " 'planet': 7412,\n",
       " 'chil': 5838,\n",
       " 'sted': 5547,\n",
       " 'qig': 4081,\n",
       " 'diet': 2115,\n",
       " 'dough': 6084,\n",
       " 'charms': 7421,\n",
       " 'bead': 2381,\n",
       " 'folder': 3459,\n",
       " 'resources': 4903,\n",
       " 'lob': 7211,\n",
       " 'secur': 6820,\n",
       " 'jujitsu': 3146,\n",
       " 'essay': 7212,\n",
       " 'scout': 3560,\n",
       " 'visiting': 5131,\n",
       " 'ahead': 4736,\n",
       " 'hra': 3675,\n",
       " 'craps': 4500,\n",
       " 'spy': 7039,\n",
       " 'collectors': 4348,\n",
       " 'scient': 4038,\n",
       " 'fet': 3979,\n",
       " 'creating': 833,\n",
       " 'eous': 7716,\n",
       " 'ca': 295,\n",
       " 'her': 972,\n",
       " 'fotomagico': 6897,\n",
       " 'oh': 7538,\n",
       " 'cramp': 7849,\n",
       " 'grapevine': 5779,\n",
       " 'weigh': 2225,\n",
       " 'cosmetics': 2143,\n",
       " 'environmental': 4406,\n",
       " 'hen': 6045,\n",
       " 'print': 1776,\n",
       " 'frequ': 3521,\n",
       " 'height': 3496,\n",
       " 'ough': 2561,\n",
       " 'beads': 2296,\n",
       " 'far': 4752,\n",
       " 'prepping': 6672,\n",
       " 'bas': 514,\n",
       " 'recreation': 2885,\n",
       " 'greeting': 5445,\n",
       " 'happen': 3300,\n",
       " 'interpre': 3980,\n",
       " 'engaging': 5034,\n",
       " 'produces': 8011,\n",
       " 'pencils': 7230,\n",
       " 'herbs': 2819,\n",
       " 'cip': 4428,\n",
       " 'leston': 7773,\n",
       " 'niqu': 313,\n",
       " 'practice': 551,\n",
       " 'cents': 6281,\n",
       " 'supervis': 6845,\n",
       " 'nick': 7244,\n",
       " 'insert': 4553,\n",
       " 'eyeshadow': 5929,\n",
       " 'sugges': 3850,\n",
       " 'interior': 2546,\n",
       " 'lowering': 7162,\n",
       " 'eyelashes': 2667,\n",
       " 'ckey': 2175,\n",
       " 'ku': 2585,\n",
       " 'pregnancy': 2955,\n",
       " 'inches': 4013,\n",
       " 'flash': 2071,\n",
       " 'skis': 8006,\n",
       " 'mart': 7132,\n",
       " 'while': 830,\n",
       " 'stor': 3644,\n",
       " 'attra': 3758,\n",
       " 'schooling': 4036,\n",
       " 'chocolate': 1879,\n",
       " 'ratatou': 4848,\n",
       " 'rinses': 8013,\n",
       " 'numerous': 7068,\n",
       " 'starters': 6963,\n",
       " 'balancing': 5058,\n",
       " 'poster': 4100,\n",
       " 'diamond': 6733,\n",
       " 'naturally': 3829,\n",
       " 'sack': 5526,\n",
       " 'warmers': 7681,\n",
       " 'lion': 3788,\n",
       " 'prior': 4335,\n",
       " 'kne': 1861,\n",
       " 'road': 1925,\n",
       " 'seaf': 3369,\n",
       " 'conclu': 6134,\n",
       " 'ison': 4367,\n",
       " 'goes': 4672,\n",
       " 'attracting': 6894,\n",
       " 'snowboard': 1173,\n",
       " 'ringe': 5469,\n",
       " 'irts': 1689,\n",
       " 'needles': 4458,\n",
       " 'composer': 4040,\n",
       " 'niest': 8094,\n",
       " 'ly': 186,\n",
       " 'north': 5220,\n",
       " 'vests': 6140,\n",
       " 'need': 607,\n",
       " 'g': 36,\n",
       " 'black': 726,\n",
       " 'telesco': 1891,\n",
       " 'catches': 5621,\n",
       " 'kart': 6318,\n",
       " 'dimensions': 6586,\n",
       " 'saying': 7576,\n",
       " 'access': 1201,\n",
       " 'mmage': 6687,\n",
       " 'plie': 6983,\n",
       " 'shopping': 3749,\n",
       " 'ony': 4982,\n",
       " 'banjo': 3909,\n",
       " 'ds': 281,\n",
       " 'race': 3567,\n",
       " 'snowballs': 6083,\n",
       " 'downlo': 2811,\n",
       " 'occur': 4065,\n",
       " 'rhy': 2036,\n",
       " 'competitive': 2942,\n",
       " 'fel': 4084,\n",
       " 'vascular': 4961,\n",
       " 'multiple': 2436,\n",
       " 'nunchucks': 2373,\n",
       " 'guards': 6667,\n",
       " 'custom': 1603,\n",
       " 'early': 4168,\n",
       " 'platform': 7273,\n",
       " 'frost': 7569,\n",
       " 'caving': 6635,\n",
       " 'interesting': 3072,\n",
       " 'door': 1053,\n",
       " 'measures': 5509,\n",
       " 'producing': 4577,\n",
       " 'mail': 2804,\n",
       " 'decorations': 3404,\n",
       " 'wel': 759,\n",
       " 'climbing': 1338,\n",
       " 'garbage': 8097,\n",
       " 'ulk': 4247,\n",
       " 'attemp': 2976,\n",
       " 'expla': 2104,\n",
       " 'standard': 5847,\n",
       " 'mobility': 7209,\n",
       " 'chemic': 2190,\n",
       " 'chiropra': 2825,\n",
       " 'portrait': 2693,\n",
       " 'call': 1980,\n",
       " 'ferred': 5259,\n",
       " 'sturdy': 7240,\n",
       " 'movies': 3710,\n",
       " 'zip': 6184,\n",
       " 'fringe': 5584,\n",
       " 'cleansing': 4358,\n",
       " 'eos': 4533,\n",
       " 'death': 7085,\n",
       " 'pair': 507,\n",
       " 'sm': 1216,\n",
       " 'cartooning': 7000,\n",
       " 'casual': 6232,\n",
       " 'hybrid': 5554,\n",
       " 'accordi': 2949,\n",
       " 'stud': 6609,\n",
       " 'walking': 2970,\n",
       " 'wan': 2601,\n",
       " 'phobias': 7704,\n",
       " 'met': 1687,\n",
       " 'rac': 6130,\n",
       " 'rail': 2853,\n",
       " 'mul': 1259,\n",
       " 'treatments': 2019,\n",
       " 'manufacture': 4517,\n",
       " 'outs': 4260,\n",
       " 'library': 4854,\n",
       " 'mug': 5462,\n",
       " 'ness': 325,\n",
       " 'foc': 2317,\n",
       " 'removed': 4049,\n",
       " 'blackhawk': 5739,\n",
       " 'gre': 7290,\n",
       " 'price': 4219,\n",
       " 'hu': 1074,\n",
       " 'locat': 5612,\n",
       " 'nic': 1495,\n",
       " 'cds': 6778,\n",
       " 'knock': 7797,\n",
       " 'red': 495,\n",
       " 'feelings': 3774,\n",
       " 'diabetes': 7112,\n",
       " 'bullets': 8039,\n",
       " 'zor': 3358,\n",
       " 'newtonian': 7070,\n",
       " 'company': 1938,\n",
       " 'ceiling': 3318,\n",
       " 'engine': 1385,\n",
       " 'ary': 677,\n",
       " 'lizards': 4917,\n",
       " 'spreadsheet': 7523,\n",
       " 'latin': 2121,\n",
       " 'tier': 6526,\n",
       " 'olive': 5611,\n",
       " 'minated': 7899,\n",
       " 'amanders': 6443,\n",
       " 'rhythmic': 4276,\n",
       " 'glitter': 5882,\n",
       " 'say': 3020,\n",
       " 'sweat': 5749,\n",
       " 'beating': 7661,\n",
       " 'tape': 2142,\n",
       " 'picadillo': 7461,\n",
       " 'ults': 3372,\n",
       " 'sharpen': 3175,\n",
       " 'examp': 6287,\n",
       " 'kiatsu': 3418,\n",
       " 'microphones': 6507,\n",
       " 'lungs': 8040,\n",
       " 'love': 1450,\n",
       " 'saw': 2904,\n",
       " 'gies': 2999,\n",
       " 'spinach': 6174,\n",
       " 'tent': 3471,\n",
       " 'zi': 3816,\n",
       " 'ctive': 1120,\n",
       " 'clicking': 2204,\n",
       " 'ceramics': 963,\n",
       " 'coordin': 3407,\n",
       " 'disable': 6952,\n",
       " 'root': 3256,\n",
       " 'shapes': 2133,\n",
       " 'cornrows': 4733,\n",
       " 'pee': 7350,\n",
       " 'normal': 4029,\n",
       " 'shampoo': 4677,\n",
       " 'curl': 3102,\n",
       " 'conditions': 3767,\n",
       " 'scratching': 3996,\n",
       " 'deo': 4185,\n",
       " 'suc': 1493,\n",
       " 'gamer': 5056,\n",
       " 'vent': 922,\n",
       " 'sense': 4912,\n",
       " 'gevity': 5074,\n",
       " 'blush': 5722,\n",
       " 'differences': 4429,\n",
       " 'communicating': 6170,\n",
       " 'oscar': 6338,\n",
       " 'recommended': 6854,\n",
       " 'roads': 7447,\n",
       " 'ister': 7721,\n",
       " 'anima': 7883,\n",
       " 'shadows': 4779,\n",
       " 'lights': 2275,\n",
       " 'lide': 6472,\n",
       " 'spent': 8091,\n",
       " 'chid': 5381,\n",
       " 'video': 80,\n",
       " 'rol': 4725,\n",
       " 'ets': 3707,\n",
       " 'bodybuilder': 6259,\n",
       " 'creole': 6570,\n",
       " 'substance': 6883,\n",
       " 'barbe': 7624,\n",
       " 'instant': 7726,\n",
       " 'ies': 270,\n",
       " 'plumber': 5707,\n",
       " 'bone': 3208,\n",
       " 'instal': 2746,\n",
       " 'utensils': 3551,\n",
       " 'vinyl': 5679,\n",
       " 'evalu': 3911,\n",
       " 'ject': 2254,\n",
       " 'crescent': 4073,\n",
       " 'walks': 8028,\n",
       " 'spirit': 4420,\n",
       " 'ure': 199,\n",
       " 'inarian': 2602,\n",
       " 'strengthe': 3009,\n",
       " 'anut': 4723,\n",
       " 'coordination': 5859,\n",
       " 'perm': 6164,\n",
       " 'brist': 6680,\n",
       " 'barri': 5418,\n",
       " 'des': 1438,\n",
       " 'alligators': 7189,\n",
       " 'insom': 3652,\n",
       " 'illu': 889,\n",
       " 'zers': 4591,\n",
       " 'tarrag': 7909,\n",
       " 'grapho': 8004,\n",
       " 'hatching': 6453,\n",
       " 'jacket': 2721,\n",
       " 'highlights': 4296,\n",
       " 'pri': 1358,\n",
       " 'boat': 2250,\n",
       " 'ink': 1140,\n",
       " 'along': 2440,\n",
       " 'trick': 505,\n",
       " 'opti': 6951,\n",
       " 'reat': 233,\n",
       " 'costumes': 4220,\n",
       " 'expen': 3085,\n",
       " 'detergent': 6760,\n",
       " 'spark': 4072,\n",
       " 'ranch': 6517,\n",
       " 'our': 248,\n",
       " 'drive': 1848,\n",
       " 'loves': 5712,\n",
       " 'hunting': 3213,\n",
       " 'kitty': 6026,\n",
       " 'spot': 3038,\n",
       " 'chemistry': 5616,\n",
       " 'buying': 1023,\n",
       " 'istopmotion': 7824,\n",
       " 'xes': 4151,\n",
       " 'selection': 3946,\n",
       " 'toothpick': 7572,\n",
       " 'perfume': 4653,\n",
       " 'train': 260,\n",
       " 'description': 7067,\n",
       " 'thirds': 5239,\n",
       " 'musician': 1213,\n",
       " 'gifts': 3848,\n",
       " 'calories': 5402,\n",
       " 'holding': 3306,\n",
       " 'needed': 1507,\n",
       " 'art': 164,\n",
       " 'crun': 2879,\n",
       " 'focaccia': 5750,\n",
       " 'tables': 2529,\n",
       " 'development': 3335,\n",
       " 'aid': 3221,\n",
       " 'bing': 7127,\n",
       " 'cancer': 4790,\n",
       " 'ung': 861,\n",
       " 'bee': 1427,\n",
       " 'next': 1809,\n",
       " 'cof': 1824,\n",
       " 'spo': 3834,\n",
       " 'iss': 4983,\n",
       " 'disk': 6051,\n",
       " 'species': 5369,\n",
       " 'considered': 5686,\n",
       " 'anging': 1170,\n",
       " 'altern': 3200,\n",
       " 'rela': 630,\n",
       " 'bathing': 7547,\n",
       " 'bur': 2344,\n",
       " 'spout': 5126,\n",
       " 'locking': 5909,\n",
       " 'angles': 3198,\n",
       " 'vered': 3606,\n",
       " 'pl': 320,\n",
       " 'pas': 4572,\n",
       " 'canvas': 2830,\n",
       " 'any': 646,\n",
       " 'pinning': 6508,\n",
       " 'brooke': 6579,\n",
       " 'compensate': 7336,\n",
       " 'stance': 1962,\n",
       " 'sivananda': 4022,\n",
       " 'psychic': 5983,\n",
       " 'download': 3403,\n",
       " 'tang': 5141,\n",
       " 'poser': 4596,\n",
       " 'amoun': 2647,\n",
       " 'gathering': 1701,\n",
       " 'blending': 6520,\n",
       " 'bmx': 2827,\n",
       " 'man': 291,\n",
       " 'float': 4486,\n",
       " 'impression': 7411,\n",
       " 'cen': 2486,\n",
       " 'officer': 3775,\n",
       " 'loosening': 7637,\n",
       " 'culinary': 2008,\n",
       " 'balloons': 7865,\n",
       " 'below': 5297,\n",
       " 'ginger': 4434,\n",
       " 'ual': 1414,\n",
       " 'plane': 5766,\n",
       " 'ys': 1443,\n",
       " 'endurance': 5258,\n",
       " 'sven': 6939,\n",
       " 'alumin': 7347,\n",
       " 'loo': 1353,\n",
       " 'fle': 907,\n",
       " 'young': 5390,\n",
       " 'strengthen': 1660,\n",
       " 'ked': 1360,\n",
       " 'adh': 4003,\n",
       " 'loun': 6735,\n",
       " 'flowers': 1433,\n",
       " 'soli': 2095,\n",
       " 'hobby': 2840,\n",
       " 'cables': 4375,\n",
       " 'suffer': 4414,\n",
       " 'improvisation': 4492,\n",
       " 'dete': 4944,\n",
       " 'conser': 4821,\n",
       " 'nationally': 3464,\n",
       " 'fright': 7880,\n",
       " 'worms': 4709,\n",
       " 'easy': 523,\n",
       " 'tooth': 2458,\n",
       " 'dies': 2776,\n",
       " 'col': 293,\n",
       " 'listen': 5951,\n",
       " 'satur': 7699,\n",
       " 'flir': 6186,\n",
       " 'donations': 8042,\n",
       " 'creation': 2366,\n",
       " 'micro': 1295,\n",
       " 'dip': 2725,\n",
       " 'hale': 6103,\n",
       " 'venti': 7508,\n",
       " 'trap': 4143,\n",
       " 'awareness': 4664,\n",
       " 'yoko': 6632,\n",
       " 'lifes': 3162,\n",
       " 'perfect': 1451,\n",
       " 'kneeling': 7176,\n",
       " 'chips': 3797,\n",
       " 'jacks': 6027,\n",
       " 'licen': 1255,\n",
       " 'pamela': 4410,\n",
       " 'career': 990,\n",
       " 'handyman': 4634,\n",
       " 'circular': 4700,\n",
       " 'birding': 6087,\n",
       " 'mir': 3214,\n",
       " 'debris': 6785,\n",
       " 'degrees': 5051,\n",
       " 'cool': 2040,\n",
       " 'ind': 1048,\n",
       " 'ents': 1008,\n",
       " 'wr': 423,\n",
       " 'placed': 4530,\n",
       " 'major': 1897,\n",
       " 'row': 2237,\n",
       " 'odds': 7563,\n",
       " 'explanation': 5326,\n",
       " 'heart': 2329,\n",
       " 'ener': 1115,\n",
       " 'folk': 5719,\n",
       " 'shield': 3994,\n",
       " 'basic': 583,\n",
       " 'potter': 579,\n",
       " 'hatha': 4641,\n",
       " 'ves': 361,\n",
       " 'landscap': 3820,\n",
       " 'ring': 753,\n",
       " 'rainbow': 5751,\n",
       " 'before': 508,\n",
       " 'ases': 3347,\n",
       " 'groove': 7183,\n",
       " 'geckos': 5452,\n",
       " 'desktop': 5684,\n",
       " 'blen': 3194,\n",
       " 'blades': 4459,\n",
       " 'mid': 1668,\n",
       " 'heads': 4249,\n",
       " 'icious': 5892,\n",
       " 'lity': 893,\n",
       " 'evenly': 5878,\n",
       " 'boil': 5870,\n",
       " 'dimen': 3309,\n",
       " 'gentle': 3782,\n",
       " 'geography': 7645,\n",
       " 'ures': 3327,\n",
       " 'ventilation': 7985,\n",
       " 'realistic': 4104,\n",
       " 'holic': 4554,\n",
       " 'celebrity': 3800,\n",
       " 'urn': 1238,\n",
       " 'feet': 1161,\n",
       " 'wanted': 4094,\n",
       " 'trouble': 5012,\n",
       " 'upper': 1436,\n",
       " 'heating': 5972,\n",
       " 'utter': 6226,\n",
       " 'construct': 7111,\n",
       " 'slight': 3806,\n",
       " 'kee': 2750,\n",
       " 'ques': 1340,\n",
       " 'dimin': 6946,\n",
       " 'luggage': 7188,\n",
       " 'sunscreen': 7224,\n",
       " 'bells': 5667,\n",
       " 'chemicals': 3232,\n",
       " 'gorgeous': 5904,\n",
       " 'buffalo': 7814,\n",
       " 'est': 378,\n",
       " 'obe': 2277,\n",
       " 'floor': 1476,\n",
       " 'sup': 4573,\n",
       " 'cia': 5251,\n",
       " 'mol': 3047,\n",
       " 'turist': 3702,\n",
       " 'beginners': 2532,\n",
       " 'false': 4421,\n",
       " 'athle': 2795,\n",
       " 'blank': 6885,\n",
       " 'ths': 2295,\n",
       " 'erro': 6783,\n",
       " 'professional': 155,\n",
       " 'dynamic': 6691,\n",
       " 'grains': 5500,\n",
       " 'handle': 1568,\n",
       " 'mushroom': 6805,\n",
       " 'labe': 7496,\n",
       " 'makeup': 431,\n",
       " 'caveats': 7702,\n",
       " 'purchased': 6926,\n",
       " 'understanding': 2883,\n",
       " 'minute': 6709,\n",
       " 'menu': 2590,\n",
       " 'loosen': 3087,\n",
       " 'airbrushing': 5651,\n",
       " 'vere': 6868,\n",
       " 'contractor': 2123,\n",
       " 'metal': 3446,\n",
       " '1970s': 7531,\n",
       " 'hold': 1492,\n",
       " 'reading': 1155,\n",
       " 'demonstra': 1700,\n",
       " 'mizing': 5465,\n",
       " 'land': 1550,\n",
       " 'recommen': 5426,\n",
       " 'titu': 4001,\n",
       " 'solve': 6761,\n",
       " 'covered': 3723,\n",
       " 'messages': 6650,\n",
       " 'rist': 2363,\n",
       " 'worker': 3442,\n",
       " 'binding': 3672,\n",
       " 'ishing': 5108,\n",
       " 'ypt': 7398,\n",
       " 'dolphin': 7644,\n",
       " 'strings': 1839,\n",
       " 'biodiesel': 4379,\n",
       " 'artisan': 5046,\n",
       " 'ool': 6319,\n",
       " 'grow': 1369,\n",
       " 'pling': 6046,\n",
       " 'tiger': 7138,\n",
       " 'juice': 3725,\n",
       " 'locate': 6167,\n",
       " 'applique': 5448,\n",
       " 'increase': 1844,\n",
       " 'dot': 7145,\n",
       " 'lots': 3864,\n",
       " 'leaves': 3889,\n",
       " 'pilates': 1083,\n",
       " 'ide': 581,\n",
       " 'boxing': 748,\n",
       " 'brake': 1903,\n",
       " 'crunches': 4095,\n",
       " 'tol': 4286,\n",
       " 'cers': 4906,\n",
       " 'synchronous': 6592,\n",
       " 'frosting': 6968,\n",
       " 'thinking': 3422,\n",
       " 'textual': 6003,\n",
       " 'graphy': 949,\n",
       " 'provide': 2511,\n",
       " 'wrapping': 2378,\n",
       " 'uté': 5731,\n",
       " 'paper': 888,\n",
       " 'raising': 6339,\n",
       " 'tical': 2587,\n",
       " 'swiss': 7063,\n",
       " 'contrast': 4374,\n",
       " 'vehicle': 2891,\n",
       " 'pant': 4231,\n",
       " 'atvs': 7448,\n",
       " 'gymnastics': 2523,\n",
       " 'pictures': 2515,\n",
       " 'rose': 4726,\n",
       " 'session': 4913,\n",
       " 'il': 161,\n",
       " 'rin': 439,\n",
       " 'proje': 1001,\n",
       " 'fauce': 5632,\n",
       " 'motiv': 7192,\n",
       " 'meters': 5189,\n",
       " 'centerpiece': 4273,\n",
       " 'arch': 1595,\n",
       " 'pianist': 4444,\n",
       " 'poison': 4889,\n",
       " 'woman': 2136,\n",
       " 'uli': 6739,\n",
       " 'tomy': 6784,\n",
       " 'sailboat': 5652,\n",
       " 'sheet': 3051,\n",
       " 'education': 1291,\n",
       " 'ash': 529,\n",
       " 'often': 1429,\n",
       " 'swe': 1978,\n",
       " 'qualities': 3831,\n",
       " 'cartridges': 4547,\n",
       " 'fru': 2661,\n",
       " 'gloss': 5856,\n",
       " 'ag': 965,\n",
       " 'reverse': 1639,\n",
       " 'c53': 7128,\n",
       " 'terrain': 6360,\n",
       " 'jogi': 8170,\n",
       " 'expression': 3491,\n",
       " 'loops': 3881,\n",
       " 'popcorn': 7907,\n",
       " 'rint': 5099,\n",
       " 'tations': 2974,\n",
       " 'pane': 6640,\n",
       " 'clothes': 1632,\n",
       " 'trumpet': 3755,\n",
       " 'seek': 7170,\n",
       " 'cure': 2422,\n",
       " 'roses': 7343,\n",
       " 'zes': 5492,\n",
       " 'namaskara': 6125,\n",
       " 'installed': 6454,\n",
       " 'radio': 3630,\n",
       " 'basis': 4525,\n",
       " 'settings': 3289,\n",
       " 'setting': 1384,\n",
       " 'qual': 1139,\n",
       " 'toilet': 3202,\n",
       " 'balls': 1406,\n",
       " 'punches': 3511,\n",
       " 'ctee': 5139,\n",
       " 'visit': 5822,\n",
       " 'replacement': 6924,\n",
       " 'what': 421,\n",
       " 'essentials': 7515,\n",
       " 'pping': 1918,\n",
       " '8': 24,\n",
       " 'minnie': 6670,\n",
       " 'hygi': 2813,\n",
       " 'dumb': 3002,\n",
       " 'sian': 4340,\n",
       " 'headstand': 6331,\n",
       " 'requi': 588,\n",
       " 'reptiles': 3781,\n",
       " 'fting': 4079,\n",
       " 'pins': 3833,\n",
       " 'careful': 4262,\n",
       " 'particular': 3714,\n",
       " 'flour': 5952,\n",
       " 'hawk': 4928,\n",
       " 'qualifications': 3346,\n",
       " 'noticed': 6006,\n",
       " 'reach': 4366,\n",
       " 'equal': 5140,\n",
       " 'be': 114,\n",
       " 'fying': 4175,\n",
       " 'expres': 2520,\n",
       " 'post': 1182,\n",
       " 'heavy': 3095,\n",
       " 'lubric': 4282,\n",
       " 'friendship': 6766,\n",
       " 'punch': 992,\n",
       " 'shrimp': 1577,\n",
       " 'strap': 3295,\n",
       " 'abdo': 7775,\n",
       " 'ster': 398,\n",
       " 'variety': 2194,\n",
       " 'touring': 5563,\n",
       " 'might': 4499,\n",
       " 'pitcher': 3362,\n",
       " 'sit': 1263,\n",
       " 'washer': 4713,\n",
       " 'bars': 3939,\n",
       " 'ctors': 5173,\n",
       " 'touch': 2410,\n",
       " 'gating': 7487,\n",
       " 'mum': 4873,\n",
       " 'shif': 7965,\n",
       " 'ple': 1324,\n",
       " 'tal': 2384,\n",
       " 'values': 4487,\n",
       " 'bartending': 6506,\n",
       " 'premi': 8160,\n",
       " 'ancial': 5101,\n",
       " 'decoupage': 7826,\n",
       " 'freel': 2267,\n",
       " 'snowball': 6543,\n",
       " 'blade': 2428,\n",
       " 'aquariums': 5323,\n",
       " 'anyone': 5505,\n",
       " 'jump': 1366,\n",
       " 'etch': 4818,\n",
       " 'anas': 4132,\n",
       " 'due': 3737,\n",
       " 'housing': 6245,\n",
       " 'curtain': 4432,\n",
       " 'tricks': 813,\n",
       " 'natal': 2446,\n",
       " 'pul': 4437,\n",
       " 'sexual': 5883,\n",
       " 'sale': 4911,\n",
       " 'valuable': 4975,\n",
       " 'coating': 7300,\n",
       " 'vap': 8142,\n",
       " 'better': 1350,\n",
       " 'on': 63,\n",
       " 'dealing': 3184,\n",
       " 'inch': 5432,\n",
       " 'greaser': 7711,\n",
       " '70': 5162,\n",
       " 'raw': 4160,\n",
       " 'combs': 8016,\n",
       " 'owners': 3540,\n",
       " 'therapist': 1564,\n",
       " 'disclosure': 8098,\n",
       " 'offic': 3348,\n",
       " 'curling': 4311,\n",
       " 'calori': 7507,\n",
       " 'sco': 2737,\n",
       " 'certain': 2385,\n",
       " 'gaz': 3172,\n",
       " 'again': 5824,\n",
       " 'accred': 7783,\n",
       " 'lli': 3340,\n",
       " 'roller': 2319,\n",
       " 'superhero': 4850,\n",
       " 'cycles': 5171,\n",
       " 'ri': 139,\n",
       " 'axe': 6423,\n",
       " 'dorm': 5830,\n",
       " 'hub': 7628,\n",
       " 'forth': 6867,\n",
       " 'literature': 4721,\n",
       " 'favorite': 3971,\n",
       " ...}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()\n",
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0213c24f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[67,\n",
       " 2769,\n",
       " 616,\n",
       " 119,\n",
       " 438,\n",
       " 305,\n",
       " 594,\n",
       " 13,\n",
       " 6248,\n",
       " 12,\n",
       " 727,\n",
       " 58,\n",
       " 73,\n",
       " 5224,\n",
       " 83,\n",
       " 7120,\n",
       " 99,\n",
       " 119,\n",
       " 1821,\n",
       " 136,\n",
       " 421,\n",
       " 119,\n",
       " 1117,\n",
       " 14,\n",
       " 1728,\n",
       " 305,\n",
       " 594,\n",
       " 13,\n",
       " 6248,\n",
       " 202,\n",
       " 342,\n",
       " 186,\n",
       " 5287,\n",
       " 1187,\n",
       " 9,\n",
       " 48,\n",
       " 5890,\n",
       " 93,\n",
       " 102,\n",
       " 92,\n",
       " 30,\n",
       " 1371,\n",
       " 3584,\n",
       " 58,\n",
       " 76,\n",
       " 77,\n",
       " 80,\n",
       " 63,\n",
       " 594,\n",
       " 13,\n",
       " 397,\n",
       " 14]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_tokenizer.encode(sm_data[23])\n",
    "#sm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c499fe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "sm_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"sm_tokenizer\")\n",
    "en_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"en_tokenizer\")\n",
    "pt_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"pt_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f294b954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def dataset_reorg(batch):\n",
    "    data_dict = {\n",
    "        'vid': {'indices': torch.arange(1,len(batch['vid'])+1),\n",
    "                'values': batch['vid']},\n",
    "        'aud': {'values': batch['aud']},\n",
    "        'en': {'tokens': en_tokenizer.encode(\" \".join(batch['en']))},\n",
    "        'pt': {'tokens': pt_tokenizer.encode(\" \".join(batch['pt']))},\n",
    "        'sm': {'tokens': sm_tokenizer.encode(\" \".join(batch['sm']) if batch['sm'] else \"\")}\n",
    "    }\n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "e8c5a191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hotfix(batch):\n",
    "    batch['vid']['indices'] = batch['vid']['indices'] + 1\n",
    "    return(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "a5248dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/184949 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#dataset = dataset.map(dataset_reorg)\n",
    "dataset = dataset.map(hotfix, num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a30cc37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "#dataset.save_to_disk('/shared/how2_all_proc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ccd6343",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk('/shared/how2_all_proc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "732a3fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.join(os.path.dirname(os.path.abspath('.'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d71a2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'encoders' from '/efs-private/multimodal/encoders.py'>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import encoders\n",
    "from encoders import MultimodalCollator\n",
    "import importlib\n",
    "\n",
    "importlib.reload(encoders)\n",
    "#importlib.reload(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2b49236",
   "metadata": {},
   "outputs": [],
   "source": [
    "modality_config = {'aud':{'type':'matrix','pad_len':2048, 'max_channels':40},\n",
    "                   'en':{'type':'sequence','pad_len':256, 'data_col_name':\"tokens\"},\n",
    "                   'pt':{'type':'sequence','pad_len':256, 'data_col_name':\"tokens\"},\n",
    "                   'sm':{'type':'sequence','pad_len':128, 'data_col_name':\"tokens\"},\n",
    "                   'vid':{'type':'sequence','pad_len':2048},\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d3febe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = MultimodalCollator(modality_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "331da530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['aud', 'en', 'pt', 'sm', 'vid'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc.modality_collators.keys()\n",
    "#sample = dataset[0]\n",
    "#sample.pop('name',None)\n",
    "#sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b4a43b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['--8pSDeC-fg_1',\n",
       " '--8pSDeC-fg_10',\n",
       " '--8pSDeC-fg_11',\n",
       " '--8pSDeC-fg_12',\n",
       " '--8pSDeC-fg_13',\n",
       " '--8pSDeC-fg_14',\n",
       " '--8pSDeC-fg_15',\n",
       " '--8pSDeC-fg_16']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = dataset[10:18]\n",
    "sample.pop('name',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22233a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "                        dataset, shuffle=True, collate_fn=mc,\n",
    "                        batch_size=8,\n",
    "                        prefetch_factor=4, num_workers=2\n",
    "                )\n",
    "dl_iter = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28faa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = BioZorroCollator(pad_len=config.pad_len, pad_token=0, attn_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06b068db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for a in train_dataloader:\n",
    "#    sample=a\n",
    "    #for k,v in a.items():\n",
    "    #    print(f\"{k}:{v.shape}\")\n",
    "    #exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1fff214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aud:torch.Size([8, 2048, 40])\n",
      "en:torch.Size([8, 256])\n",
      "en:torch.Size([8, 256])\n",
      "pt:torch.Size([8, 256])\n",
      "pt:torch.Size([8, 256])\n",
      "sm:torch.Size([8, 128])\n",
      "sm:torch.Size([8, 128])\n",
      "vid:torch.Size([8, 2048])\n",
      "vid:torch.Size([8, 2048])\n",
      "vid:torch.Size([8, 2048])\n",
      "aud:torch.Size([8, 2048, 40])\n",
      "en:torch.Size([8, 256])\n",
      "en:torch.Size([8, 256])\n",
      "pt:torch.Size([8, 256])\n",
      "pt:torch.Size([8, 256])\n",
      "sm:torch.Size([8, 128])\n",
      "sm:torch.Size([8, 128])\n",
      "vid:torch.Size([8, 2048])\n",
      "vid:torch.Size([8, 2048])\n",
      "vid:torch.Size([8, 2048])\n",
      "aud:torch.Size([8, 2048, 40])\n",
      "en:torch.Size([8, 256])\n",
      "en:torch.Size([8, 256])\n",
      "pt:torch.Size([8, 256])\n",
      "pt:torch.Size([8, 256])\n",
      "sm:torch.Size([8, 128])\n",
      "sm:torch.Size([8, 128])\n",
      "vid:torch.Size([8, 2048])\n",
      "vid:torch.Size([8, 2048])\n",
      "vid:torch.Size([8, 2048])\n",
      "aud:torch.Size([8, 2048, 40])\n",
      "en:torch.Size([8, 256])\n",
      "en:torch.Size([8, 256])\n",
      "pt:torch.Size([8, 256])\n",
      "pt:torch.Size([8, 256])\n",
      "sm:torch.Size([8, 128])\n",
      "sm:torch.Size([8, 128])\n",
      "vid:torch.Size([8, 2048])\n",
      "vid:torch.Size([8, 2048])\n",
      "vid:torch.Size([8, 2048])\n",
      "aud:torch.Size([8, 2048, 40])\n",
      "en:torch.Size([8, 256])\n",
      "en:torch.Size([8, 256])\n",
      "pt:torch.Size([8, 256])\n",
      "pt:torch.Size([8, 256])\n",
      "sm:torch.Size([8, 128])\n",
      "sm:torch.Size([8, 128])\n",
      "vid:torch.Size([8, 2048])\n",
      "vid:torch.Size([8, 2048])\n",
      "vid:torch.Size([8, 2048])\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    sample = next(dl_iter)\n",
    "    for k,v in sample.items():\n",
    "        for v2 in v.values():\n",
    "            print(f\"{k}:{v2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c16faba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aud': {'values': tensor([[[ 1.0293e+01,  1.4074e+01,  1.5368e+01,  ...,  1.4706e+01,\n",
       "             1.4404e+01,  1.3697e+01],\n",
       "           [ 1.2080e+01,  1.4280e+01,  1.6147e+01,  ...,  1.4706e+01,\n",
       "             1.4404e+01,  1.3389e+01],\n",
       "           [ 1.1578e+01,  1.3397e+01,  1.6225e+01,  ...,  1.5894e+01,\n",
       "             1.5503e+01,  1.4995e+01],\n",
       "           ...,\n",
       "           [-1.0000e+04, -1.0000e+04, -1.0000e+04,  ..., -1.0000e+04,\n",
       "            -1.0000e+04, -1.0000e+04],\n",
       "           [-1.0000e+04, -1.0000e+04, -1.0000e+04,  ..., -1.0000e+04,\n",
       "            -1.0000e+04, -1.0000e+04],\n",
       "           [-1.0000e+04, -1.0000e+04, -1.0000e+04,  ..., -1.0000e+04,\n",
       "            -1.0000e+04, -1.0000e+04]],\n",
       "  \n",
       "          [[ 1.1876e+01,  1.4209e+01,  1.7955e+01,  ...,  1.8331e+01,\n",
       "             1.9452e+01,  1.8965e+01],\n",
       "           [ 1.3558e+01,  1.5207e+01,  1.9663e+01,  ...,  1.8095e+01,\n",
       "             2.0407e+01,  2.0452e+01],\n",
       "           [ 1.3151e+01,  1.5642e+01,  2.0220e+01,  ...,  1.8656e+01,\n",
       "             1.9929e+01,  2.0085e+01],\n",
       "           ...,\n",
       "           [-1.0000e+04, -1.0000e+04, -1.0000e+04,  ..., -1.0000e+04,\n",
       "            -1.0000e+04, -1.0000e+04],\n",
       "           [-1.0000e+04, -1.0000e+04, -1.0000e+04,  ..., -1.0000e+04,\n",
       "            -1.0000e+04, -1.0000e+04],\n",
       "           [-1.0000e+04, -1.0000e+04, -1.0000e+04,  ..., -1.0000e+04,\n",
       "            -1.0000e+04, -1.0000e+04]],\n",
       "  \n",
       "          [[ 9.5476e+00,  1.0731e+01,  1.1283e+01,  ...,  1.1066e+01,\n",
       "             1.1479e+01,  1.0675e+01],\n",
       "           [ 7.6543e+00,  7.9813e+00,  7.2155e+00,  ...,  1.0702e+01,\n",
       "             1.0825e+01,  1.0339e+01],\n",
       "           [ 8.8544e+00,  9.2034e+00,  8.2674e+00,  ...,  1.0742e+01,\n",
       "             1.0680e+01,  1.1044e+01],\n",
       "           ...,\n",
       "           [-1.0000e+04, -1.0000e+04, -1.0000e+04,  ..., -1.0000e+04,\n",
       "            -1.0000e+04, -1.0000e+04],\n",
       "           [-1.0000e+04, -1.0000e+04, -1.0000e+04,  ..., -1.0000e+04,\n",
       "            -1.0000e+04, -1.0000e+04],\n",
       "           [-1.0000e+04, -1.0000e+04, -1.0000e+04,  ..., -1.0000e+04,\n",
       "            -1.0000e+04, -1.0000e+04]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[ 1.1357e+01,  1.4020e+01,  1.4404e+01,  ...,  1.4537e+01,\n",
       "             1.5020e+01,  1.4323e+01],\n",
       "           [ 1.1239e+01,  1.4480e+01,  1.6135e+01,  ...,  1.4502e+01,\n",
       "             1.4988e+01,  1.5007e+01],\n",
       "           [ 1.1175e+01,  1.5207e+01,  1.8588e+01,  ...,  1.5736e+01,\n",
       "             1.6494e+01,  1.6648e+01],\n",
       "           ...,\n",
       "           [-1.0000e+04, -1.0000e+04, -1.0000e+04,  ..., -1.0000e+04,\n",
       "            -1.0000e+04, -1.0000e+04],\n",
       "           [-1.0000e+04, -1.0000e+04, -1.0000e+04,  ..., -1.0000e+04,\n",
       "            -1.0000e+04, -1.0000e+04],\n",
       "           [-1.0000e+04, -1.0000e+04, -1.0000e+04,  ..., -1.0000e+04,\n",
       "            -1.0000e+04, -1.0000e+04]],\n",
       "  \n",
       "          [[ 1.2926e+01,  1.5078e+01,  1.6541e+01,  ...,  1.6423e+01,\n",
       "             1.6341e+01,  1.5742e+01],\n",
       "           [ 1.4055e+01,  1.6920e+01,  1.9111e+01,  ...,  1.6423e+01,\n",
       "             1.5325e+01,  1.4649e+01],\n",
       "           [ 1.2437e+01,  1.6694e+01,  1.9129e+01,  ...,  1.8429e+01,\n",
       "             1.7483e+01,  1.3665e+01],\n",
       "           ...,\n",
       "           [-1.0000e+04, -1.0000e+04, -1.0000e+04,  ..., -1.0000e+04,\n",
       "            -1.0000e+04, -1.0000e+04],\n",
       "           [-1.0000e+04, -1.0000e+04, -1.0000e+04,  ..., -1.0000e+04,\n",
       "            -1.0000e+04, -1.0000e+04],\n",
       "           [-1.0000e+04, -1.0000e+04, -1.0000e+04,  ..., -1.0000e+04,\n",
       "            -1.0000e+04, -1.0000e+04]],\n",
       "  \n",
       "          [[ 1.6160e+01,  1.6984e+01,  1.6295e+01,  ...,  1.4976e+01,\n",
       "             1.5392e+01,  1.5496e+01],\n",
       "           [ 1.6618e+01,  1.7506e+01,  1.7193e+01,  ...,  1.3690e+01,\n",
       "             1.4158e+01,  1.4456e+01],\n",
       "           [ 1.6972e+01,  1.7945e+01,  1.7826e+01,  ...,  1.3925e+01,\n",
       "             1.3720e+01,  1.3403e+01],\n",
       "           ...,\n",
       "           [-1.0000e+04, -1.0000e+04, -1.0000e+04,  ..., -1.0000e+04,\n",
       "            -1.0000e+04, -1.0000e+04],\n",
       "           [-1.0000e+04, -1.0000e+04, -1.0000e+04,  ..., -1.0000e+04,\n",
       "            -1.0000e+04, -1.0000e+04],\n",
       "           [-1.0000e+04, -1.0000e+04, -1.0000e+04,  ..., -1.0000e+04,\n",
       "            -1.0000e+04, -1.0000e+04]]])},\n",
       " 'en': {'tokens': tensor([[ 184,  133,   11,  ...,    0,    0,    0],\n",
       "          [3668,   18,    0,  ...,    0,    0,    0],\n",
       "          [5580,  133, 2446,  ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [2131,  546, 2111,  ...,    0,    0,    0],\n",
       "          [1148,  422,  222,  ...,    0,    0,    0],\n",
       "          [ 226,   11,  111,  ...,    0,    0,    0]]),\n",
       "  'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 1,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1]])},\n",
       " 'pt': {'tokens': tensor([[ 216,   16,  648,  ...,    0,    0,    0],\n",
       "          [  35, 6496,   18,  ...,    0,    0,    0],\n",
       "          [4799,  144,  190,  ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [2284,  330, 2379,  ...,    0,    0,    0],\n",
       "          [1279,  648, 1363,  ...,    0,    0,    0],\n",
       "          [ 261,  237,  424,  ...,    0,    0,    0]]),\n",
       "  'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1]])},\n",
       " 'sm': {'tokens': tensor([[  90,   99,   67,  ...,    0,    0,    0],\n",
       "          [  67,  606,   30,  ...,    0,    0,    0],\n",
       "          [1504,  721,  638,  ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [  99,   67,  182,  ...,    0,    0,    0],\n",
       "          [6182, 4734,  213,  ...,    0,    0,    0],\n",
       "          [  93, 1081,  107,  ...,    0,    0,    0]]),\n",
       "  'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1]])},\n",
       " 'vid': {'indices': tensor([[   0,    1,    2,  ..., 2045, 2046, 2047],\n",
       "          [   0,    1,    2,  ..., 2045, 2046, 2047],\n",
       "          [   0,    1,    2,  ..., 2045, 2046, 2047],\n",
       "          ...,\n",
       "          [   0,    1,    2,  ..., 2045, 2046, 2047],\n",
       "          [   0,    1,    2,  ..., 2045, 2046, 2047],\n",
       "          [   0,    1,    2,  ..., 2045, 2046, 2047]]),\n",
       "  'attention_mask': tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
       "          [1, 0, 0,  ..., 0, 0, 0],\n",
       "          [1, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [1, 0, 0,  ..., 0, 0, 0],\n",
       "          [1, 0, 0,  ..., 0, 0, 0],\n",
       "          [1, 0, 0,  ..., 0, 0, 0]]),\n",
       "  'values': tensor([[0.7642, 0.3913, 0.3996,  ..., 0.2251, 0.0737, 0.3345],\n",
       "          [0.1077, 0.0610, 0.0390,  ..., 0.1719, 0.0248, 0.5372],\n",
       "          [0.3224, 0.5431, 0.2412,  ..., 0.1837, 0.1387, 0.3121],\n",
       "          ...,\n",
       "          [0.2602, 0.6704, 0.3152,  ..., 0.1646, 0.2372, 0.8170],\n",
       "          [0.0697, 0.1877, 0.2092,  ..., 0.1904, 0.0322, 0.2160],\n",
       "          [0.5115, 0.1549, 0.4784,  ..., 0.0765, 0.1239, 0.6813]])}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97495a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoders import PatchEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcb727a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "penc=PatchEncoder(max_tokens=320, patch_size = (32,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e26bfab3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x,at = penc(collated['aud'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "137fb14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    14.1159,     16.2858,     15.9622,  ...,     13.2830,\n",
       "             13.0229,     12.6879],\n",
       "        [    14.3077,     16.7415,     16.5961,  ...,     13.9218,\n",
       "             12.8430,     12.2267],\n",
       "        [    15.1387,     17.2530,     17.0790,  ...,     13.9710,\n",
       "             13.5673,     12.5957],\n",
       "        ...,\n",
       "        [-10000.0000, -10000.0000, -10000.0000,  ..., -10000.0000,\n",
       "         -10000.0000, -10000.0000],\n",
       "        [-10000.0000, -10000.0000, -10000.0000,  ..., -10000.0000,\n",
       "         -10000.0000, -10000.0000],\n",
       "        [-10000.0000, -10000.0000, -10000.0000,  ..., -10000.0000,\n",
       "         -10000.0000, -10000.0000]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collated['aud']['values'][6,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c7a016a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbatch\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maud\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"
     ]
    }
   ],
   "source": [
    "batch['aud']['values'][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "bb9d1d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2048, 40])"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collated['aud']['values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "ae978338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5fda222d90>"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10dc1af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_configs = {'aud':{'type':'PatchEncoder',\n",
    "                         'max_tokens':320, \n",
    "                         'patch_size':(32,8)},\n",
    "                   'en':{'type':'SequenceEncoder',\n",
    "                         'num_embeddings':8192, \n",
    "                         'max_tokens':256},\n",
    "                   'pt':{'type':'SequenceEncoder',\n",
    "                         'num_embeddings':8192, \n",
    "                         'max_tokens':256},\n",
    "                   'sm':{'type':'SequenceEncoder',\n",
    "                         'num_embeddings':8192, \n",
    "                         'max_tokens':256},\n",
    "                   'vid':{'type':'TabularEncoder',\n",
    "                          \"num_embeddings\":2048,\n",
    "                          'max_tokens':2048,\n",
    "                         },\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c39c6727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoders import encoders_dict\n",
    "encoder_classes = {modality_name: encoders_dict[encoder_config['type']](**encoder_config)\n",
    "                         for modality_name, encoder_config in encoder_configs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d356455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_classes = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b0f1126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': tensor([[ 936,  646,  118,  ...,    0,    0,    0],\n",
      "        [2787,  113,  553,  ...,    0,    0,    0],\n",
      "        [ 366,  144,  113,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [3020,  434,  159,  ...,    0,    0,    0],\n",
      "        [ 284,   11,   83,  ...,    0,    0,    0],\n",
      "        [ 330,  118, 2625,  ...,    0,    0,    0]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]])}\n",
      "{'tokens': tensor([[1350,  771,  119,  ...,    0,    0,    0],\n",
      "        [ 261,  227,  561,  ...,    0,    0,    0],\n",
      "        [ 381,  360,  156,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [1759,  472,  177,  ...,    0,    0,    0],\n",
      "        [ 107,  316,  557,  ...,    0,    0,    0],\n",
      "        [ 643,  119,   80,  ...,    0,    0,    0]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]])}\n",
      "{'tokens': tensor([[  67, 2769,  616,  ...,    0,    0,    0],\n",
      "        [  67, 2769,  616,  ...,    0,    0,    0],\n",
      "        [  67, 2769,  616,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  67, 2769,  616,  ...,    0,    0,    0],\n",
      "        [  67, 2769,  616,  ...,    0,    0,    0],\n",
      "        [  67, 2769,  616,  ...,    0,    0,    0]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "token_types = list(collated.keys())\n",
    "tokens, attention_masks = zip(*[encoder_classes[modality_name](collated[modality_name])\n",
    "                   for modality_name in token_types])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58a5b2b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.8065, -0.7556,  1.8182,  ...,  0.5336,  0.1504, -0.0399],\n",
       "          [-0.6251, -0.6853,  1.7297,  ...,  0.4453,  0.1072,  0.0351],\n",
       "          [-1.3333,  0.0306,  1.6114,  ...,  1.3236, -0.1118, -0.2603],\n",
       "          ...,\n",
       "          [-1.2580, -0.3370,  1.8117,  ...,  0.9178,  0.1335, -0.2491],\n",
       "          [-1.3704, -0.1506,  1.9158,  ...,  1.0557,  0.0300, -0.2704],\n",
       "          [-1.4346, -0.0863,  1.7615,  ...,  1.1182, -0.0207, -0.2369]],\n",
       " \n",
       "         [[-0.8065, -0.7556,  1.8182,  ...,  0.5336,  0.1504, -0.0399],\n",
       "          [-0.6251, -0.6853,  1.7297,  ...,  0.4453,  0.1072,  0.0351],\n",
       "          [-1.3333,  0.0306,  1.6114,  ...,  1.3236, -0.1118, -0.2603],\n",
       "          ...,\n",
       "          [-1.2580, -0.3370,  1.8117,  ...,  0.9178,  0.1335, -0.2491],\n",
       "          [-1.3704, -0.1506,  1.9158,  ...,  1.0557,  0.0300, -0.2704],\n",
       "          [-1.4346, -0.0863,  1.7615,  ...,  1.1182, -0.0207, -0.2369]],\n",
       " \n",
       "         [[-0.8065, -0.7556,  1.8182,  ...,  0.5336,  0.1504, -0.0399],\n",
       "          [-0.6251, -0.6853,  1.7297,  ...,  0.4453,  0.1072,  0.0351],\n",
       "          [-1.3333,  0.0306,  1.6114,  ...,  1.3236, -0.1118, -0.2603],\n",
       "          ...,\n",
       "          [-1.2580, -0.3370,  1.8117,  ...,  0.9178,  0.1335, -0.2491],\n",
       "          [-1.3704, -0.1506,  1.9158,  ...,  1.0557,  0.0300, -0.2704],\n",
       "          [-1.4346, -0.0863,  1.7615,  ...,  1.1182, -0.0207, -0.2369]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.8065, -0.7556,  1.8182,  ...,  0.5336,  0.1504, -0.0399],\n",
       "          [-0.6251, -0.6853,  1.7297,  ...,  0.4453,  0.1072,  0.0351],\n",
       "          [-1.3333,  0.0306,  1.6114,  ...,  1.3236, -0.1118, -0.2603],\n",
       "          ...,\n",
       "          [-1.2580, -0.3370,  1.8117,  ...,  0.9178,  0.1335, -0.2491],\n",
       "          [-1.3704, -0.1506,  1.9158,  ...,  1.0557,  0.0300, -0.2704],\n",
       "          [-1.4346, -0.0863,  1.7615,  ...,  1.1182, -0.0207, -0.2369]],\n",
       " \n",
       "         [[-0.8065, -0.7556,  1.8182,  ...,  0.5336,  0.1504, -0.0399],\n",
       "          [-0.6251, -0.6853,  1.7297,  ...,  0.4453,  0.1072,  0.0351],\n",
       "          [-1.3333,  0.0306,  1.6114,  ...,  1.3236, -0.1118, -0.2603],\n",
       "          ...,\n",
       "          [-1.2580, -0.3370,  1.8117,  ...,  0.9178,  0.1335, -0.2491],\n",
       "          [-1.3704, -0.1506,  1.9158,  ...,  1.0557,  0.0300, -0.2704],\n",
       "          [-1.4346, -0.0863,  1.7615,  ...,  1.1182, -0.0207, -0.2369]],\n",
       " \n",
       "         [[-0.8065, -0.7556,  1.8182,  ...,  0.5336,  0.1504, -0.0399],\n",
       "          [-0.6251, -0.6853,  1.7297,  ...,  0.4453,  0.1072,  0.0351],\n",
       "          [-1.3333,  0.0306,  1.6114,  ...,  1.3236, -0.1118, -0.2603],\n",
       "          ...,\n",
       "          [-1.2580, -0.3370,  1.8117,  ...,  0.9178,  0.1335, -0.2491],\n",
       "          [-1.3704, -0.1506,  1.9158,  ...,  1.0557,  0.0300, -0.2704],\n",
       "          [-1.4346, -0.0863,  1.7615,  ...,  1.1182, -0.0207, -0.2369]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 6.9128e-02,  9.7821e-01, -6.2327e-02,  ...,  9.7263e-01,\n",
       "           -8.2704e-02,  9.9168e-01],\n",
       "          [ 4.2805e-02,  9.4951e-01, -1.0715e-02,  ...,  1.0356e+00,\n",
       "           -1.4866e-02,  1.0056e+00],\n",
       "          [ 6.7793e-03,  1.0162e+00, -2.1934e-02,  ...,  1.0048e+00,\n",
       "            1.9161e-02,  9.0646e-01],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "            0.0000e+00,  1.0000e+00],\n",
       "          [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "            0.0000e+00,  1.0000e+00],\n",
       "          [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "            0.0000e+00,  1.0000e+00]],\n",
       " \n",
       "         [[ 8.5939e-01,  5.7583e-01,  8.9450e-01,  ...,  9.8822e-01,\n",
       "           -9.6422e-02,  9.6829e-01],\n",
       "          [ 8.5573e-01,  5.8682e-01,  8.4696e-01,  ...,  1.0092e+00,\n",
       "            3.3094e-02,  1.0089e+00],\n",
       "          [ 7.8875e-01,  5.1826e-01,  8.3587e-01,  ...,  1.1250e+00,\n",
       "           -6.8064e-02,  8.8002e-01],\n",
       "          ...,\n",
       "          [ 8.4147e-01,  5.4030e-01,  8.2186e-01,  ...,  1.0000e+00,\n",
       "            1.0366e-04,  1.0000e+00],\n",
       "          [ 8.4147e-01,  5.4030e-01,  8.2186e-01,  ...,  1.0000e+00,\n",
       "            1.0366e-04,  1.0000e+00],\n",
       "          [ 8.4147e-01,  5.4030e-01,  8.2186e-01,  ...,  1.0000e+00,\n",
       "            1.0366e-04,  1.0000e+00]],\n",
       " \n",
       "         [[ 9.3814e-01, -3.6389e-01,  9.1456e-01,  ...,  9.2754e-01,\n",
       "            8.5434e-02,  1.0507e+00],\n",
       "          [ 9.3495e-01, -4.7187e-01,  9.6565e-01,  ...,  9.4838e-01,\n",
       "            2.9068e-02,  9.7558e-01],\n",
       "          [ 9.2356e-01, -3.6963e-01,  9.6152e-01,  ...,  1.0092e+00,\n",
       "            3.3198e-02,  1.0089e+00],\n",
       "          ...,\n",
       "          [ 9.0930e-01, -4.1615e-01,  9.3641e-01,  ...,  1.0000e+00,\n",
       "            2.0733e-04,  1.0000e+00],\n",
       "          [ 9.0930e-01, -4.1615e-01,  9.3641e-01,  ...,  1.0000e+00,\n",
       "            2.0733e-04,  1.0000e+00],\n",
       "          [ 9.0930e-01, -4.1615e-01,  9.3641e-01,  ...,  1.0000e+00,\n",
       "            2.0733e-04,  1.0000e+00]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-9.8745e-01,  3.6095e-01, -1.0132e+00,  ...,  1.0306e+00,\n",
       "           -5.0405e-02,  1.0232e+00],\n",
       "          [-9.5779e-01,  2.8874e-01, -1.0070e+00,  ...,  9.7288e-01,\n",
       "            2.5830e-03,  1.0250e+00],\n",
       "          [-9.7134e-01,  3.0728e-01, -9.5137e-01,  ...,  9.5452e-01,\n",
       "            2.9392e-02,  1.0935e+00],\n",
       "          ...,\n",
       "          [-9.5892e-01,  2.8366e-01, -9.9385e-01,  ...,  1.0000e+00,\n",
       "            5.1832e-04,  1.0000e+00],\n",
       "          [-9.5892e-01,  2.8366e-01, -9.9385e-01,  ...,  1.0000e+00,\n",
       "            5.1832e-04,  1.0000e+00],\n",
       "          [-9.5892e-01,  2.8366e-01, -9.9385e-01,  ...,  1.0000e+00,\n",
       "            5.1832e-04,  1.0000e+00]],\n",
       " \n",
       "         [[-2.9026e-01,  9.4623e-01, -3.9725e-01,  ...,  9.4819e-01,\n",
       "           -1.0182e-02,  9.2757e-01],\n",
       "          [-2.8145e-01,  9.6247e-01, -4.4347e-01,  ...,  9.2966e-01,\n",
       "            9.2791e-02,  1.0365e+00],\n",
       "          [-3.1896e-01,  1.0505e+00, -5.2904e-01,  ...,  9.5340e-01,\n",
       "           -1.3888e-02,  1.0314e+00],\n",
       "          ...,\n",
       "          [-2.7942e-01,  9.6017e-01, -4.7522e-01,  ...,  1.0000e+00,\n",
       "            6.2198e-04,  1.0000e+00],\n",
       "          [-2.7942e-01,  9.6017e-01, -4.7522e-01,  ...,  1.0000e+00,\n",
       "            6.2198e-04,  1.0000e+00],\n",
       "          [-2.7942e-01,  9.6017e-01, -4.7522e-01,  ...,  1.0000e+00,\n",
       "            6.2198e-04,  1.0000e+00]],\n",
       " \n",
       "         [[ 6.8817e-01,  8.0953e-01,  4.2301e-01,  ...,  1.0549e+00,\n",
       "           -7.6698e-02,  9.9629e-01],\n",
       "          [ 6.6377e-01,  7.7015e-01,  4.3046e-01,  ...,  1.0048e+00,\n",
       "            1.9887e-02,  9.0646e-01],\n",
       "          [ 6.6262e-01,  6.8407e-01,  4.8784e-01,  ...,  9.8122e-01,\n",
       "            1.9984e-03,  1.0263e+00],\n",
       "          ...,\n",
       "          [ 6.5699e-01,  7.5390e-01,  4.5239e-01,  ...,  1.0000e+00,\n",
       "            7.2564e-04,  1.0000e+00],\n",
       "          [ 6.5699e-01,  7.5390e-01,  4.5239e-01,  ...,  1.0000e+00,\n",
       "            7.2564e-04,  1.0000e+00],\n",
       "          [ 6.5699e-01,  7.5390e-01,  4.5239e-01,  ...,  1.0000e+00,\n",
       "            7.2564e-04,  1.0000e+00]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[-2.5621e-02,  9.5252e-01,  1.7055e-02,  ...,  9.3490e-01,\n",
       "            8.7116e-03,  9.0265e-01],\n",
       "          [-4.3550e-02,  1.0376e+00, -1.5690e-02,  ...,  9.8856e-01,\n",
       "            1.3412e-02,  1.0635e+00],\n",
       "          [ 3.1655e-02,  1.0072e+00, -2.8070e-02,  ...,  1.0739e+00,\n",
       "            2.7628e-02,  9.6746e-01],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "            0.0000e+00,  1.0000e+00],\n",
       "          [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "            0.0000e+00,  1.0000e+00],\n",
       "          [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "            0.0000e+00,  1.0000e+00]],\n",
       " \n",
       "         [[ 8.4653e-01,  4.7748e-01,  8.2166e-01,  ...,  1.0559e+00,\n",
       "            2.0094e-02,  1.0202e+00],\n",
       "          [ 8.6458e-01,  6.2474e-01,  8.9045e-01,  ...,  9.9874e-01,\n",
       "            6.2234e-02,  1.0107e+00],\n",
       "          [ 7.9847e-01,  5.5445e-01,  8.0821e-01,  ...,  9.8894e-01,\n",
       "           -6.2162e-02,  1.0566e+00],\n",
       "          ...,\n",
       "          [ 8.4147e-01,  5.4030e-01,  8.2186e-01,  ...,  1.0000e+00,\n",
       "            1.0366e-04,  1.0000e+00],\n",
       "          [ 8.4147e-01,  5.4030e-01,  8.2186e-01,  ...,  1.0000e+00,\n",
       "            1.0366e-04,  1.0000e+00],\n",
       "          [ 8.4147e-01,  5.4030e-01,  8.2186e-01,  ...,  1.0000e+00,\n",
       "            1.0366e-04,  1.0000e+00]],\n",
       " \n",
       "         [[ 9.0346e-01, -3.8199e-01,  9.0971e-01,  ...,  1.0262e+00,\n",
       "            1.3833e-02,  9.9738e-01],\n",
       "          [ 8.7003e-01, -3.1688e-01,  9.8649e-01,  ...,  9.6687e-01,\n",
       "            3.2895e-02,  1.0358e+00],\n",
       "          [ 9.7779e-01, -5.0082e-01,  9.6995e-01,  ...,  1.0069e+00,\n",
       "            5.3020e-02,  1.0427e+00],\n",
       "          ...,\n",
       "          [ 9.0930e-01, -4.1615e-01,  9.3641e-01,  ...,  1.0000e+00,\n",
       "            2.0733e-04,  1.0000e+00],\n",
       "          [ 9.0930e-01, -4.1615e-01,  9.3641e-01,  ...,  1.0000e+00,\n",
       "            2.0733e-04,  1.0000e+00],\n",
       "          [ 9.0930e-01, -4.1615e-01,  9.3641e-01,  ...,  1.0000e+00,\n",
       "            2.0733e-04,  1.0000e+00]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-9.5625e-01,  1.6634e-01, -9.9691e-01,  ...,  1.0323e+00,\n",
       "            6.9570e-02,  1.0581e+00],\n",
       "          [-1.0248e+00,  3.2540e-01, -1.0499e+00,  ...,  1.0079e+00,\n",
       "           -1.0195e-02,  9.6251e-01],\n",
       "          [-9.3046e-01,  2.1132e-01, -9.4617e-01,  ...,  1.0649e+00,\n",
       "            6.9685e-02,  1.0376e+00],\n",
       "          ...,\n",
       "          [-9.5892e-01,  2.8366e-01, -9.9385e-01,  ...,  1.0000e+00,\n",
       "            5.1832e-04,  1.0000e+00],\n",
       "          [-9.5892e-01,  2.8366e-01, -9.9385e-01,  ...,  1.0000e+00,\n",
       "            5.1832e-04,  1.0000e+00],\n",
       "          [-9.5892e-01,  2.8366e-01, -9.9385e-01,  ...,  1.0000e+00,\n",
       "            5.1832e-04,  1.0000e+00]],\n",
       " \n",
       "         [[-3.2102e-01,  9.2060e-01, -4.7108e-01,  ...,  9.6361e-01,\n",
       "           -1.0467e-02,  1.0092e+00],\n",
       "          [-2.8711e-01,  9.8720e-01, -4.5026e-01,  ...,  1.0890e+00,\n",
       "           -7.5850e-03,  1.0588e+00],\n",
       "          [-3.3925e-01,  9.5792e-01, -4.2464e-01,  ...,  9.4269e-01,\n",
       "            7.0013e-02,  9.8203e-01],\n",
       "          ...,\n",
       "          [-2.7942e-01,  9.6017e-01, -4.7522e-01,  ...,  1.0000e+00,\n",
       "            6.2198e-04,  1.0000e+00],\n",
       "          [-2.7942e-01,  9.6017e-01, -4.7522e-01,  ...,  1.0000e+00,\n",
       "            6.2198e-04,  1.0000e+00],\n",
       "          [-2.7942e-01,  9.6017e-01, -4.7522e-01,  ...,  1.0000e+00,\n",
       "            6.2198e-04,  1.0000e+00]],\n",
       " \n",
       "         [[ 5.9493e-01,  8.4528e-01,  3.9020e-01,  ...,  1.0633e+00,\n",
       "            1.2681e-02,  9.7866e-01],\n",
       "          [ 6.8864e-01,  7.6113e-01,  4.2432e-01,  ...,  1.0739e+00,\n",
       "            2.8353e-02,  9.6746e-01],\n",
       "          [ 6.7511e-01,  7.0583e-01,  3.1987e-01,  ...,  1.0272e+00,\n",
       "           -3.1238e-02,  1.0032e+00],\n",
       "          ...,\n",
       "          [ 6.5699e-01,  7.5390e-01,  4.5239e-01,  ...,  1.0000e+00,\n",
       "            7.2564e-04,  1.0000e+00],\n",
       "          [ 6.5699e-01,  7.5390e-01,  4.5239e-01,  ...,  1.0000e+00,\n",
       "            7.2564e-04,  1.0000e+00],\n",
       "          [ 6.5699e-01,  7.5390e-01,  4.5239e-01,  ...,  1.0000e+00,\n",
       "            7.2564e-04,  1.0000e+00]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[-2.2277e-02,  1.0001e+00,  4.0344e-02,  ...,  9.3100e-01,\n",
       "           -9.5179e-02,  9.5789e-01],\n",
       "          [ 2.3395e-02,  1.0454e+00, -2.4090e-02,  ...,  9.9425e-01,\n",
       "           -4.1320e-02,  1.0785e+00],\n",
       "          [ 2.7679e-02,  9.2840e-01, -1.9970e-03,  ...,  1.0524e+00,\n",
       "            4.2526e-02,  1.0874e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "            0.0000e+00,  1.0000e+00],\n",
       "          [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "            0.0000e+00,  1.0000e+00],\n",
       "          [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "            0.0000e+00,  1.0000e+00]],\n",
       " \n",
       "         [[ 8.1919e-01,  5.4040e-01,  8.6220e-01,  ...,  9.3100e-01,\n",
       "           -9.5075e-02,  9.5789e-01],\n",
       "          [ 8.6487e-01,  5.8575e-01,  7.9777e-01,  ...,  9.9425e-01,\n",
       "           -4.1217e-02,  1.0785e+00],\n",
       "          [ 8.6915e-01,  4.6870e-01,  8.1986e-01,  ...,  1.0524e+00,\n",
       "            4.2629e-02,  1.0874e+00],\n",
       "          ...,\n",
       "          [ 8.4147e-01,  5.4030e-01,  8.2186e-01,  ...,  1.0000e+00,\n",
       "            1.0366e-04,  1.0000e+00],\n",
       "          [ 8.4147e-01,  5.4030e-01,  8.2186e-01,  ...,  1.0000e+00,\n",
       "            1.0366e-04,  1.0000e+00],\n",
       "          [ 8.4147e-01,  5.4030e-01,  8.2186e-01,  ...,  1.0000e+00,\n",
       "            1.0366e-04,  1.0000e+00]],\n",
       " \n",
       "         [[ 8.8702e-01, -4.1605e-01,  9.7676e-01,  ...,  9.3100e-01,\n",
       "           -9.4972e-02,  9.5789e-01],\n",
       "          [ 9.3269e-01, -3.7070e-01,  9.1232e-01,  ...,  9.9425e-01,\n",
       "           -4.1113e-02,  1.0785e+00],\n",
       "          [ 9.3698e-01, -4.8775e-01,  9.3442e-01,  ...,  1.0524e+00,\n",
       "            4.2733e-02,  1.0874e+00],\n",
       "          ...,\n",
       "          [ 9.0930e-01, -4.1615e-01,  9.3641e-01,  ...,  1.0000e+00,\n",
       "            2.0733e-04,  1.0000e+00],\n",
       "          [ 9.0930e-01, -4.1615e-01,  9.3641e-01,  ...,  1.0000e+00,\n",
       "            2.0733e-04,  1.0000e+00],\n",
       "          [ 9.0930e-01, -4.1615e-01,  9.3641e-01,  ...,  1.0000e+00,\n",
       "            2.0733e-04,  1.0000e+00]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-9.8120e-01,  2.8376e-01, -9.5351e-01,  ...,  9.3100e-01,\n",
       "           -9.4661e-02,  9.5789e-01],\n",
       "          [-9.3553e-01,  3.2911e-01, -1.0179e+00,  ...,  9.9425e-01,\n",
       "           -4.0802e-02,  1.0785e+00],\n",
       "          [-9.3125e-01,  2.1206e-01, -9.9585e-01,  ...,  1.0524e+00,\n",
       "            4.3044e-02,  1.0874e+00],\n",
       "          ...,\n",
       "          [-9.5892e-01,  2.8366e-01, -9.9385e-01,  ...,  1.0000e+00,\n",
       "            5.1832e-04,  1.0000e+00],\n",
       "          [-9.5892e-01,  2.8366e-01, -9.9385e-01,  ...,  1.0000e+00,\n",
       "            5.1832e-04,  1.0000e+00],\n",
       "          [-9.5892e-01,  2.8366e-01, -9.9385e-01,  ...,  1.0000e+00,\n",
       "            5.1832e-04,  1.0000e+00]],\n",
       " \n",
       "         [[-3.0169e-01,  9.6027e-01, -4.3488e-01,  ...,  9.3100e-01,\n",
       "           -9.4557e-02,  9.5789e-01],\n",
       "          [-2.5602e-01,  1.0056e+00, -4.9931e-01,  ...,  9.9425e-01,\n",
       "           -4.0698e-02,  1.0785e+00],\n",
       "          [-2.5174e-01,  8.8857e-01, -4.7722e-01,  ...,  1.0524e+00,\n",
       "            4.3148e-02,  1.0874e+00],\n",
       "          ...,\n",
       "          [-2.7942e-01,  9.6017e-01, -4.7522e-01,  ...,  1.0000e+00,\n",
       "            6.2198e-04,  1.0000e+00],\n",
       "          [-2.7942e-01,  9.6017e-01, -4.7522e-01,  ...,  1.0000e+00,\n",
       "            6.2198e-04,  1.0000e+00],\n",
       "          [-2.7942e-01,  9.6017e-01, -4.7522e-01,  ...,  1.0000e+00,\n",
       "            6.2198e-04,  1.0000e+00]],\n",
       " \n",
       "         [[ 6.3471e-01,  7.5400e-01,  4.9274e-01,  ...,  9.3100e-01,\n",
       "           -9.4453e-02,  9.5789e-01],\n",
       "          [ 6.8038e-01,  7.9935e-01,  4.2830e-01,  ...,  9.9425e-01,\n",
       "           -4.0595e-02,  1.0785e+00],\n",
       "          [ 6.8467e-01,  6.8230e-01,  4.5040e-01,  ...,  1.0524e+00,\n",
       "            4.3251e-02,  1.0874e+00],\n",
       "          ...,\n",
       "          [ 6.5699e-01,  7.5390e-01,  4.5239e-01,  ...,  1.0000e+00,\n",
       "            7.2564e-04,  1.0000e+00],\n",
       "          [ 6.5699e-01,  7.5390e-01,  4.5239e-01,  ...,  1.0000e+00,\n",
       "            7.2564e-04,  1.0000e+00],\n",
       "          [ 6.5699e-01,  7.5390e-01,  4.5239e-01,  ...,  1.0000e+00,\n",
       "            7.2564e-04,  1.0000e+00]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[ 1.0835e+00,  3.4941e-01,  1.1488e+00,  ...,  1.1233e+00,\n",
       "            3.0009e-02,  1.7306e+00],\n",
       "          [-1.5432e+00, -0.0000e+00,  2.3733e-01,  ...,  0.0000e+00,\n",
       "           -2.1688e+00, -1.3821e+00],\n",
       "          [-2.6256e+00,  3.5385e-01,  4.8859e-01,  ...,  1.0316e+00,\n",
       "           -9.6627e-01, -5.8897e-01],\n",
       "          ...,\n",
       "          [ 2.9937e-01,  9.8508e-01, -8.4378e-01,  ...,  3.7508e-03,\n",
       "           -8.3484e-01,  7.8490e-03],\n",
       "          [ 1.1593e+00,  1.5001e-01, -3.1846e-01,  ...,  1.1992e+00,\n",
       "           -8.8540e-01,  1.9492e+00],\n",
       "          [ 2.2428e+00,  5.9927e-01,  9.7659e-01,  ...,  0.0000e+00,\n",
       "           -2.5587e+00,  1.0795e+00]],\n",
       " \n",
       "         [[ 3.6304e+00,  1.1712e+00, -2.4226e-01,  ...,  1.3671e+00,\n",
       "           -1.7242e+00,  4.3380e-01],\n",
       "          [-3.5768e+00, -2.7406e+00,  2.0468e+00,  ...,  3.0188e+00,\n",
       "           -2.2729e+00, -6.7249e-01],\n",
       "          [-0.0000e+00,  1.7926e+00,  2.8641e-02,  ...,  8.4787e-01,\n",
       "           -0.0000e+00, -3.3417e-01],\n",
       "          ...,\n",
       "          [ 2.9937e-01,  9.8508e-01, -0.0000e+00,  ...,  3.7508e-03,\n",
       "           -8.3484e-01,  7.8490e-03],\n",
       "          [ 1.1593e+00,  1.5001e-01, -3.1846e-01,  ...,  1.1992e+00,\n",
       "           -8.8540e-01,  1.9492e+00],\n",
       "          [ 2.2428e+00,  0.0000e+00,  9.7659e-01,  ...,  6.6324e-01,\n",
       "           -2.5587e+00,  1.0795e+00]],\n",
       " \n",
       "         [[ 2.7827e+00,  1.6179e+00,  1.0826e-01,  ...,  2.6462e+00,\n",
       "           -0.0000e+00, -4.3162e-01],\n",
       "          [-0.0000e+00, -1.9733e+00,  1.6092e-01,  ...,  3.1590e+00,\n",
       "           -2.2763e+00, -2.4021e+00],\n",
       "          [ 5.6754e-01, -6.3409e-01,  0.0000e+00,  ...,  3.8709e+00,\n",
       "           -1.8167e+00, -3.2296e+00],\n",
       "          ...,\n",
       "          [ 2.9937e-01,  9.8508e-01, -8.4378e-01,  ...,  3.7508e-03,\n",
       "           -8.3484e-01,  7.8490e-03],\n",
       "          [ 1.1593e+00,  1.5001e-01, -0.0000e+00,  ...,  0.0000e+00,\n",
       "           -8.8540e-01,  1.9492e+00],\n",
       "          [ 2.2428e+00,  5.9927e-01,  9.7659e-01,  ...,  6.6324e-01,\n",
       "           -2.5587e+00,  1.0795e+00]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 3.0427e+00,  4.4127e-01,  3.6451e-03,  ...,  1.7989e+00,\n",
       "           -3.5077e-01,  7.1011e-01],\n",
       "          [-1.0861e+00,  5.6056e-01,  0.0000e+00,  ...,  2.6014e+00,\n",
       "           -2.4403e+00, -1.5450e+00],\n",
       "          [-6.3522e-01,  1.0426e+00,  4.2565e-01,  ...,  2.7277e+00,\n",
       "           -8.6127e-01, -3.2907e+00],\n",
       "          ...,\n",
       "          [ 2.9937e-01,  9.8508e-01, -8.4378e-01,  ...,  3.7508e-03,\n",
       "           -8.3484e-01,  7.8490e-03],\n",
       "          [ 1.1593e+00,  1.5001e-01, -3.1846e-01,  ...,  1.1992e+00,\n",
       "           -8.8540e-01,  1.9492e+00],\n",
       "          [ 2.2428e+00,  5.9927e-01,  9.7659e-01,  ...,  6.6324e-01,\n",
       "           -2.5587e+00,  1.0795e+00]],\n",
       " \n",
       "         [[ 4.6990e-01, -7.1338e-01,  0.0000e+00,  ...,  1.6749e+00,\n",
       "            3.9543e-02,  2.4386e-01],\n",
       "          [-1.4013e+00, -2.9662e+00,  4.3123e-01,  ...,  2.6469e+00,\n",
       "           -2.6904e+00, -1.5260e+00],\n",
       "          [-2.7666e-01, -0.0000e+00,  2.1553e-02,  ...,  3.6565e+00,\n",
       "           -8.3782e-01, -2.9584e+00],\n",
       "          ...,\n",
       "          [ 2.9937e-01,  9.8508e-01, -8.4378e-01,  ...,  3.7508e-03,\n",
       "           -8.3484e-01,  7.8490e-03],\n",
       "          [ 1.1593e+00,  1.5001e-01, -3.1846e-01,  ...,  1.1992e+00,\n",
       "           -8.8540e-01,  1.9492e+00],\n",
       "          [ 2.2428e+00,  5.9927e-01,  9.7659e-01,  ...,  6.6324e-01,\n",
       "           -2.5587e+00,  1.0795e+00]],\n",
       " \n",
       "         [[ 1.4322e+00, -4.4110e-01,  0.0000e+00,  ...,  3.9185e-01,\n",
       "            5.5449e-01,  2.3495e+00],\n",
       "          [-1.2138e+00, -3.5126e+00,  1.4624e+00,  ...,  0.0000e+00,\n",
       "           -1.8083e+00, -3.9587e-01],\n",
       "          [ 3.8931e-01, -7.8228e-01,  1.0227e+00,  ...,  2.4808e+00,\n",
       "           -1.1103e+00, -1.1181e+00],\n",
       "          ...,\n",
       "          [ 2.9937e-01,  9.8508e-01, -8.4378e-01,  ...,  0.0000e+00,\n",
       "           -8.3484e-01,  0.0000e+00],\n",
       "          [ 1.1593e+00,  1.5001e-01, -3.1846e-01,  ...,  1.1992e+00,\n",
       "           -8.8540e-01,  1.9492e+00],\n",
       "          [ 2.2428e+00,  5.9927e-01,  9.7659e-01,  ...,  6.6324e-01,\n",
       "           -2.5587e+00,  1.0795e+00]]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "053a58ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2048, 512])"
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9458a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'encoders' from '/efs-private/multimodal/encoders.py'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from encoders import MultimodalCollator\n",
    "import model\n",
    "from model import MFDOOM\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "importlib.reload(encoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "cb1657b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmultimodal.modules.losses.contrastive_loss_with_temperature import ContrastiveLossWithTemperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "88f677d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torchmultimodal'"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchmultimodal.__package__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c424ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{frozenset({'en', 'aud'}): ContrastiveLossWithTemperature(), frozenset({'pt', 'aud'}): ContrastiveLossWithTemperature(), frozenset({'sm', 'aud'}): ContrastiveLossWithTemperature(), frozenset({'vid', 'aud'}): ContrastiveLossWithTemperature(), frozenset({'en', 'pt'}): ContrastiveLossWithTemperature(), frozenset({'en', 'sm'}): ContrastiveLossWithTemperature(), frozenset({'en', 'vid'}): ContrastiveLossWithTemperature(), frozenset({'sm', 'pt'}): ContrastiveLossWithTemperature(), frozenset({'vid', 'pt'}): ContrastiveLossWithTemperature(), frozenset({'vid', 'sm'}): ContrastiveLossWithTemperature()}\n",
      "Token dims without fusions: 3136\n",
      "torch.Size([3152])\n",
      "torch.Size([3152, 3152])\n"
     ]
    }
   ],
   "source": [
    "doom = MFDOOM(encoder_configs, 512,4, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71a3ee1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': tensor([[ 936,  646,  118,  ...,    0,    0,    0],\n",
      "        [2787,  113,  553,  ...,    0,    0,    0],\n",
      "        [ 366,  144,  113,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [3020,  434,  159,  ...,    0,    0,    0],\n",
      "        [ 284,   11,   83,  ...,    0,    0,    0],\n",
      "        [ 330,  118, 2625,  ...,    0,    0,    0]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]])}\n",
      "{'tokens': tensor([[1350,  771,  119,  ...,    0,    0,    0],\n",
      "        [ 261,  227,  561,  ...,    0,    0,    0],\n",
      "        [ 381,  360,  156,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [1759,  472,  177,  ...,    0,    0,    0],\n",
      "        [ 107,  316,  557,  ...,    0,    0,    0],\n",
      "        [ 643,  119,   80,  ...,    0,    0,    0]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]])}\n",
      "{'tokens': tensor([[  67, 2769,  616,  ...,    0,    0,    0],\n",
      "        [  67, 2769,  616,  ...,    0,    0,    0],\n",
      "        [  67, 2769,  616,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  67, 2769,  616,  ...,    0,    0,    0],\n",
      "        [  67, 2769,  616,  ...,    0,    0,    0],\n",
      "        [  67, 2769,  616,  ...,    0,    0,    0]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]])}\n",
      "torch.Size([8, 320, 512])\n",
      "torch.Size([8, 256, 512])\n",
      "torch.Size([8, 256, 512])\n",
      "torch.Size([8, 128, 512])\n",
      "torch.Size([8, 2048, 512])\n",
      "vid-indices-torch.Size([8, 2048])\n",
      "vid-attention_mask-torch.Size([8, 2048])\n",
      "vid-values-torch.Size([8, 2048])\n",
      "en-tokens-torch.Size([8, 256])\n",
      "en-attention_mask-torch.Size([8, 256])\n",
      "pt-tokens-torch.Size([8, 256])\n",
      "pt-attention_mask-torch.Size([8, 256])\n",
      "sm-tokens-torch.Size([8, 128])\n",
      "sm-attention_mask-torch.Size([8, 128])\n",
      "aud-values-torch.Size([8, 2048, 40])\n",
      "torch.Size([8, 3024, 512]) - torch.Size([8, 3024]) - torch.Size([3152, 3152])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3152) must match the size of tensor b (3024) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdoom\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollated\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/shared/miniconda3/envs/stp/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/efs-private/multimodal/model.py:276\u001b[0m, in \u001b[0;36mMFDOOM.forward\u001b[0;34m(self, batch, no_loss)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# Run model\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# attend and feedforward\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 276\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzorro_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(tokens)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# pooling\u001b[39;00m\n",
      "File \u001b[0;32m/shared/miniconda3/envs/stp/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/efs-private/multimodal/model.py:122\u001b[0m, in \u001b[0;36mMFDOOMLayer.forward\u001b[0;34m(self, batch, zorro_mask, padding_mask)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, zorro_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, padding_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 122\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzorro_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m batch\n\u001b[1;32m    123\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff(batch) \u001b[38;5;241m+\u001b[39m batch\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "File \u001b[0;32m/shared/miniconda3/envs/stp/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/efs-private/multimodal/model.py:102\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x, context, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m     99\u001b[0m sim \u001b[38;5;241m=\u001b[39m einsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb h i d, b h j d -> b h i j\u001b[39m\u001b[38;5;124m'\u001b[39m, q, k)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exists(attn_mask):\n\u001b[0;32m--> 102\u001b[0m     sim \u001b[38;5;241m=\u001b[39m \u001b[43msim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43msim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exists(key_padding_mask):\n\u001b[1;32m    104\u001b[0m     key_padding_mask \u001b[38;5;241m=\u001b[39m repeat(key_padding_mask, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb i -> b h j i\u001b[39m\u001b[38;5;124m\"\u001b[39m, h\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads, j\u001b[38;5;241m=\u001b[39msim\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3152) must match the size of tensor b (3024) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "doom(collated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac61e3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3024"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2048+256+256+128+320+16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58c5aed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stp",
   "language": "python",
   "name": "stp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
