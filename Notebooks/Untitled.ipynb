{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d496d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/miniconda3/envs/stp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (â€¦)lve/main/config.json: 1.55kB [00:00, 3.28MB/s]\n",
      "Downloading pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 805M/805M [00:17<00:00, 46.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "from perceiver.model.text import mlm  # auto-class registration\n",
    "\n",
    "# Name of pretrained Perceiver IO masked language model\n",
    "repo_id = \"krasserm/perceiver-io-mlm\"\n",
    "\n",
    "# Load pretrained model (ðŸ¤— wrapper)\n",
    "model = AutoModelForMaskedLM.from_pretrained(repo_id)\n",
    "assert type(model) == mlm.PerceiverMaskedLanguageModel\n",
    "\n",
    "# Access to backend model\n",
    "backend_model = model.backend_model\n",
    "assert type(backend_model) == mlm.MaskedLanguageModel\n",
    "\n",
    "# Access to backend config\n",
    "backend_config = backend_model.config\n",
    "assert backend_config == model.config.backend_config\n",
    "\n",
    "# Create Lightning wrapper from backend config and load pretrained weights\n",
    "lit_model = mlm.LitMaskedLanguageModel.create(backend_config, params=repo_id)\n",
    "\n",
    "# Access to backend model from Lightning wrapper\n",
    "backend_model = lit_model.backend_model\n",
    "\n",
    "# Create randomly initialized backend model\n",
    "backend_model_rand_init = mlm.MaskedLanguageModel(backend_config)\n",
    "\n",
    "# Create ðŸ¤— wrapper with randomly initialized backend model\n",
    "model_rand_init = mlm.PerceiverMaskedLanguageModel(mlm.PerceiverMaskedLanguageModelConfig(backend_config))\n",
    "\n",
    "# Create Lightning wrapper with randomly initialized backend model\n",
    "lit_model_rand_init = mlm.LitMaskedLanguageModel.create(backend_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21cac67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LitMaskedLanguageModel(\n",
       "  (loss): CrossEntropyLoss()\n",
       "  (model): MaskedLanguageModel(\n",
       "    (0): TextEncoder(\n",
       "      (latent_provider): TrainableQueryProvider()\n",
       "      (input_adapter): TokenInputAdapter(\n",
       "        (txt_embedding): Embedding(262, 768)\n",
       "        (pos_embedding): Embedding(2048, 768)\n",
       "      )\n",
       "      (cross_attn_1): CrossAttentionLayer(\n",
       "        (0): Residual(\n",
       "          (module): CrossAttention(\n",
       "            (q_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (kv_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention): MultiHeadAttention(\n",
       "              (q_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "              (k_proj): Linear(in_features=768, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=1280, bias=True)\n",
       "              (o_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (module): MLP(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (self_attn_1): SelfAttentionBlock(\n",
       "        (0): SelfAttentionLayer(\n",
       "          (0): Residual(\n",
       "            (module): SelfAttention(\n",
       "              (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): MultiHeadAttention(\n",
       "                (q_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (k_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (o_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (module): MLP(\n",
       "              (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SelfAttentionLayer(\n",
       "          (0): Residual(\n",
       "            (module): SelfAttention(\n",
       "              (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): MultiHeadAttention(\n",
       "                (q_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (k_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (o_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (module): MLP(\n",
       "              (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): SelfAttentionLayer(\n",
       "          (0): Residual(\n",
       "            (module): SelfAttention(\n",
       "              (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): MultiHeadAttention(\n",
       "                (q_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (k_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (o_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (module): MLP(\n",
       "              (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): SelfAttentionLayer(\n",
       "          (0): Residual(\n",
       "            (module): SelfAttention(\n",
       "              (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): MultiHeadAttention(\n",
       "                (q_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (k_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (o_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (module): MLP(\n",
       "              (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): SelfAttentionLayer(\n",
       "          (0): Residual(\n",
       "            (module): SelfAttention(\n",
       "              (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): MultiHeadAttention(\n",
       "                (q_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (k_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (o_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (module): MLP(\n",
       "              (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): SelfAttentionLayer(\n",
       "          (0): Residual(\n",
       "            (module): SelfAttention(\n",
       "              (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): MultiHeadAttention(\n",
       "                (q_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (k_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (o_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (module): MLP(\n",
       "              (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): SelfAttentionLayer(\n",
       "          (0): Residual(\n",
       "            (module): SelfAttention(\n",
       "              (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): MultiHeadAttention(\n",
       "                (q_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (k_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (o_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (module): MLP(\n",
       "              (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): SelfAttentionLayer(\n",
       "          (0): Residual(\n",
       "            (module): SelfAttention(\n",
       "              (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): MultiHeadAttention(\n",
       "                (q_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (k_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (o_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (module): MLP(\n",
       "              (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): SelfAttentionLayer(\n",
       "          (0): Residual(\n",
       "            (module): SelfAttention(\n",
       "              (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): MultiHeadAttention(\n",
       "                (q_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (k_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (o_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (module): MLP(\n",
       "              (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): SelfAttentionLayer(\n",
       "          (0): Residual(\n",
       "            (module): SelfAttention(\n",
       "              (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): MultiHeadAttention(\n",
       "                (q_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (k_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (o_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (module): MLP(\n",
       "              (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): SelfAttentionLayer(\n",
       "          (0): Residual(\n",
       "            (module): SelfAttention(\n",
       "              (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): MultiHeadAttention(\n",
       "                (q_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (k_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (o_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (module): MLP(\n",
       "              (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): SelfAttentionLayer(\n",
       "          (0): Residual(\n",
       "            (module): SelfAttention(\n",
       "              (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): MultiHeadAttention(\n",
       "                (q_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (k_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (o_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (module): MLP(\n",
       "              (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): SelfAttentionLayer(\n",
       "          (0): Residual(\n",
       "            (module): SelfAttention(\n",
       "              (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): MultiHeadAttention(\n",
       "                (q_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (k_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (o_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (module): MLP(\n",
       "              (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): SelfAttentionLayer(\n",
       "          (0): Residual(\n",
       "            (module): SelfAttention(\n",
       "              (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): MultiHeadAttention(\n",
       "                (q_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (k_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (o_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (module): MLP(\n",
       "              (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): SelfAttentionLayer(\n",
       "          (0): Residual(\n",
       "            (module): SelfAttention(\n",
       "              (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): MultiHeadAttention(\n",
       "                (q_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (k_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (o_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (module): MLP(\n",
       "              (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): SelfAttentionLayer(\n",
       "          (0): Residual(\n",
       "            (module): SelfAttention(\n",
       "              (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): MultiHeadAttention(\n",
       "                (q_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (k_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (o_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (module): MLP(\n",
       "              (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): SelfAttentionLayer(\n",
       "          (0): Residual(\n",
       "            (module): SelfAttention(\n",
       "              (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): MultiHeadAttention(\n",
       "                (q_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (k_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (o_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (module): MLP(\n",
       "              (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): SelfAttentionLayer(\n",
       "          (0): Residual(\n",
       "            (module): SelfAttention(\n",
       "              (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): MultiHeadAttention(\n",
       "                (q_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (k_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (o_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (module): MLP(\n",
       "              (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): SelfAttentionLayer(\n",
       "          (0): Residual(\n",
       "            (module): SelfAttention(\n",
       "              (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): MultiHeadAttention(\n",
       "                (q_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (k_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (o_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (module): MLP(\n",
       "              (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): SelfAttentionLayer(\n",
       "          (0): Residual(\n",
       "            (module): SelfAttention(\n",
       "              (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): MultiHeadAttention(\n",
       "                (q_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (k_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (o_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (module): MLP(\n",
       "              (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): SelfAttentionLayer(\n",
       "          (0): Residual(\n",
       "            (module): SelfAttention(\n",
       "              (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): MultiHeadAttention(\n",
       "                (q_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (k_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (o_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (module): MLP(\n",
       "              (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): SelfAttentionLayer(\n",
       "          (0): Residual(\n",
       "            (module): SelfAttention(\n",
       "              (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): MultiHeadAttention(\n",
       "                (q_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (k_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (o_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (module): MLP(\n",
       "              (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): SelfAttentionLayer(\n",
       "          (0): Residual(\n",
       "            (module): SelfAttention(\n",
       "              (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): MultiHeadAttention(\n",
       "                (q_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (k_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (o_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (module): MLP(\n",
       "              (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): SelfAttentionLayer(\n",
       "          (0): Residual(\n",
       "            (module): SelfAttention(\n",
       "              (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): MultiHeadAttention(\n",
       "                (q_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (k_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (o_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (module): MLP(\n",
       "              (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (24): SelfAttentionLayer(\n",
       "          (0): Residual(\n",
       "            (module): SelfAttention(\n",
       "              (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): MultiHeadAttention(\n",
       "                (q_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (k_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (o_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (module): MLP(\n",
       "              (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (25): SelfAttentionLayer(\n",
       "          (0): Residual(\n",
       "            (module): SelfAttention(\n",
       "              (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): MultiHeadAttention(\n",
       "                (q_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (k_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "                (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (o_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (module): MLP(\n",
       "              (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): PerceiverDecoder(\n",
       "      (output_query_provider): TrainableQueryProvider()\n",
       "      (output_adapter): TiedTokenOutputAdapter()\n",
       "      (cross_attn): CrossAttentionLayer(\n",
       "        (0): CrossAttention(\n",
       "          (q_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (kv_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=768, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=1280, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=1280, out_features=768, bias=True)\n",
       "            (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (module): MLP(\n",
       "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_model_rand_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228350b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:stp]",
   "language": "python",
   "name": "conda-env-stp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
