encoder_configs:
  aud: {type: 'PatchEncoder', max_tokens: 320, patch_size: [32,8]}
  en: {type: 'SequenceEncoder', num_embeddings: 8192, max_tokens: 64}
  pt: {type: 'SequenceEncoder', num_embeddings: 8192, max_tokens: 64}
  sm: {type: 'SequenceEncoder', num_embeddings: 8192, max_tokens: 64}
  vid: {type: 'TabularEncoder', num_embeddings: 2048, max_tokens: 2048, padding_idx: -1}
modality_config:
  aud: {type: 'matrix', pad_len: 2048, max_channels: 40,
        dropout: 0.3, predrop:true, padding:{'values': -10000}}
  en: {type: 'sequence', pad_len: 64, data_col_name: "tokens",
       dropout: 0.3, predrop:true, padding:{'tokens': 0, 'attention_mask': 1}}
  pt: {type: 'sequence', pad_len: 64, data_col_name: "tokens",
       dropout: 0.3, predrop:true, padding:{'tokens': 0, 'attention_mask': 1}}
  sm: {type: 'sequence', pad_len: 64, data_col_name: "tokens",
       dropout: 0.3, predrop:true, padding:{'tokens': 0, 'attention_mask': 1}}
  vid: {type: 'sequence', pad_len: 2048,
        dropout: 0.3, predrop:true, padding: {'indices': -1, 'values': 0, 'attention_mask': 1}}
num_fusion_tokens: 192
ds_frac: 0.01
predrop: true
batch_size: 8
lr: 0.00001
layers: 4 #12
lr_scheduler_type: "constant_with_warmup"
epochs: 64
#start_epoch: 22
inverse_doom: true 
zorro: true
#output_dir: "training_output_22_02_13_12_2023"
#restart: "training_output_22_02_13_12_2023/21"
#restart_wandb: "0l846v73"
clip: 2.0
dataset: "/shared/how2_all_proc"
