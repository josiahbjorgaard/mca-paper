encoder_configs:
  cr: {type: 'EmbeddedSequenceEncoder', input_size: 74, max_tokens: 1500}
  fa: {type: 'EmbeddedSequenceEncoder', input_size: 35, max_tokens: 450}
  of: {type: 'EmbeddedSequenceEncoder', input_size: 713, max_tokens: 450}
  gv: {type: 'EmbeddedSequenceEncoder', input_size: 300, max_tokens: 50}
modality_config:
  cr: {type: 'embedded_sequence', pad_len: 1500, data_col_name: "tokens"}
  fa: {type: 'embedded_sequence', pad_len: 450,  data_col_name: "tokens",}
  of: {type: 'embedded_sequence', pad_len: 450, data_col_name: "tokens",}
  gv: {type: 'embedded_sequence', pad_len: 50, data_col_name: "tokens",}
num_fusion_tokens: 104
ds_frac: 0.2
predrop: false
batch_size: 6
lr: 0.00001
layers: 7 #12
lr_scheduler_type: "constant_with_warmup"
epochs: 128
#start_epoch: 22
bimodal_contrastive: False
non_fusion_fcl: False
fcl: True
fcl_root: [0,1,2,3]
fusion_combos: [4,3,2]
loss_masking: True
#inverse_doom: true
zorro: false
#output_dir: "training_output_22_02_13_12_2023"
#restart: "training_output_22_02_13_12_2023/21"
#restart_wandb: "0l846v73"
clip: 2.0
dataset: "/shared/how2_all_proc_10p"
